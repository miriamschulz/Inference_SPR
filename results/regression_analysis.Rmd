---
title: "SPR results: regression-based analysis of the reading times"
author: "Miriam Schulz"
date: "10/11/2022"
output:
  html_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      collapse = TRUE,
                      fig.height = 6, fig.width = 9)
```


# About

This script contains an rRT analysis of the inference reading time data (SPR1) based on the rERP approach in Brouwer, Delogu & Crocker (2020). 


# Read data and preprocess

In this section:

- read in the data
- remove outliers based on 
    - SPR (word-by-word) RTs
    - context sentence RTs
- remove incorrectly answered trials (plausibility ratings)
- add log-transformed RTs
- standardize predictors

```{r include = FALSE}
# Workspace and libraries
rm(list = ls())
library(tidyverse)
library(lme4)              # for lmer()
library(lmerTest)          # for summary(lmer.model)
library(broom)             # for tidy(modeloutput)
library(gridExtra)         # for grid.arrange()
library(grid)              # for textGrob()
library(RColorBrewer)      # for color palettes
library(pander)            # for pander(df)
source("functions.R")      # custom preprocessing and helper functions
source("functions_rRT.R")  # rRT functions
options(dplyr.summarise.inform = FALSE)  # remove message in output
```

Read in data and remove fillers, practice and non-critical regions:

```{r message=FALSE}
# Read data
df <- read.csv("results_reads.csv", header = TRUE)

# Exclude filler and practice trials
df <- removeFillersPractice(df)

# Exclude non-critical regions
df <- removeRegions(df, keep.regions=-1:2)
```

Next, the raw data are visualized with histograms and QQ plots before removing outliers.

Histograms and residual plots:

```{r echo=FALSE, fig.width=10, fig.height=10}
par(mfrow=c(2,2))
# Histograms

# Histogram of RTs:
hist(df$RT,
     breaks = 50,
     col='steelblue',
     xlab = "RTs (ms)",
     main='Untransformed RTs')

# Histogram of log RTs :
hist(log(df$RT),
     breaks = 50,
     col='steelblue',
     xlab = "logRTs (ms)",
     main='Log-transformed RTs')

# QQ plots

# Untransformed RTs
qqnorm(df$RT,
       col='steelblue',
       pch=16,
       main='QQ plot of untransformed RTs')

# Log-transformed RTs
qqnorm(log(df$RT),
       col='steelblue',
       pch=16,
       main='QQ plot of log-transformed RTs')
# Save plot
ggsave("./plots/RT_hist_QQ_preDataExclusion.pdf", plot = last_plot(),
       width = 10, height = 10, dpi = 300)
par(mfrow=c(1,1))  # reset
```

## Remove SPR outliers

```{r message=FALSE}
# Optional: remove items with low accuracy in B
#df <- filter(df, ! Item %in% c(5,13,25))

# Remove outliers: SPR (word RTs) - SD
df <- removeOutliersSPR(df,
                        method = "sd",
                        sd.value = 4,
                         entire.trial = TRUE,
                        regions = -1:2)
```

Histograms and residual plots *after* removing SPR outliers:

```{r echo=FALSE, fig.width=10, fig.height=10}
par(mfrow=c(2,2))
# Histograms

# Histogram of RTs:
hist(df$RT,
     breaks = 50,
     col='steelblue',
     xlab = "RTs (ms)",
    main='Untransformed RTs')

# Histogram of log RTs :
hist(log(df$RT),
     breaks = 50,
     col='steelblue',
     xlab = "logRTs (ms)",
     main='Log-transformed RTs')

# QQ plots

# Untransformed RTs
qqnorm(df$RT,
       col='steelblue',
       pch=16,
       main='QQ plot of untransformed RTs')

# Log-transformed RTs
qqnorm(log(df$RT),
       col='steelblue',
       pch=16,
       main='QQ plot of log-transformed RTs')
# Save plot
ggsave("./plots/RT_hist_QQ_postDataExclusion.pdf", plot = last_plot(),
       width = 10, height = 10, dpi = 300)
par(mfrow=c(1,1))  # reset
```

Box-Cox plot:

```{r echo=FALSE, fig.width=4, fig.height=4}
# Boxcox plot
MASS::boxcox(df$RT ~ df$Cond)
```

Normality test (Shapiro-Wilk):

```{r}
# Untransformed RTs:
shapiro.test(filter(df, Region >= 0)$RT)

# Log-transformed RTs:
shapiro.test(log(filter(df, Region >= 0)$RT))
```

## Remove context sentence RT outliers and incorrectly answered trials

```{r fig.width=10, fig.height=5}
# Remove outliers: context sentence reading times
df <- removeOutliersContext(df, min.rt=500, max.rt=30000)

# Remove trials with incorrect plausibility ratings
df <- removeIncorrect(df)

# Visualize context sentence reading times with a histogram:
par(mfrow=(c(1,2)))
hist(
    filter(df, Region == 0)$ContextRT,
    breaks = 50,
    main = "Untransformed Context Sentence RTs",
    xlab = "Context RTs (ms)",
    col = "steelblue"
    # main = NULL
)
hist(
    log(filter(df, Region == 0)$ContextRT),
    breaks = 50,
    main = "Log-transformed Context Sentence RTs",
    xlab = "Context RTs (log-ms)",
    col = "steelblue"
)
par(mfrow=(c(1,1))) # reset
```

```{r echo=FALSE, fig.width=4, fig.height=3.5}
# Visualize average context reading times by condition:
cond.means.contextRTs <- aggregMeans(df, as.formula(ContextRT ~ Cond))
round(cond.means.contextRTs$Mean)
round(cond.means.contextRTs$SD)

p <-
  cond.means.contextRTs %>% 
  ggplot(aes(x = Cond, y = Mean, fill=Cond)) + 
  geom_bar(stat='identity', width=0.8) + 
  geom_errorbar(
    aes(ymin = Mean - SE, ymax = Mean + SE),
    width = 0.15,
    linewidth = 0.3
  ) +
  ylim(0, 3000) +
  xlab("Condition") +
  ylab("RT (ms)") +
  scale_fill_manual(values = c("cornflowerblue", "chartreuse3",
                               "tomato2", "darkgoldenrod1")) +
  theme_minimal() +
  theme(legend.position="None",
        text = element_text(size = 13))
filename <- paste0("./plots/", "context_RT.pdf")
ggsave(filename, plot = p, width = 3, height = 3)
p + ggtitle("Context sentence RT by condition")
```

## Further preprocessing: transformations etc.

In this subsection: 

- Rename variables
- Transform to factor when necessary
- Combine Inference+Coherence ratings into a new combined predictor with PCA, extracting the first component
- add precritical reading times as predictors 
- add log-transformed versions of the reading times (the SPR RT (DV) as well as predictors (IVs))
- standardize the predictors

```{r message=FALSE}
# Rename variables 
rating_colnames <- str_remove(names(df), "Rating")
names(df) <- rating_colnames
df <- rename(df, ProdNorms = ProductionPercentage)

# Transform variables to factors
df$Cond <- as.factor(df$Cond)
df$Item <- as.factor(as.numeric(as.character(df$Item)))
df$Subject <- as.factor(as.numeric(as.character(df$Subject)))

# Transform the binary plausibility predictor to numeric, then to factor
df$Plausible <- as.factor(ifelse(df$Plausible == "Yes", 1, 0))

# Better: combine the predictors using PCA
pca_fit <- stats::prcomp(dplyr::select(df, Inference, Coherence), scale=TRUE)
df$InfCoPCA <- pca_fit$x[,1]

# Extract precritical RT -1 and add to df:
df.precrit1 <- filter(df, Region == -1) %>% 
  dplyr::select(Item, Cond, Subject, RT) %>% 
  rename(RT_precrit1 = RT)
df <- merge(df, df.precrit1, by = c("Item", "Cond", "Subject"))

# Extract precritical RT -2 and add to df:
#df.precrit2 <- filter(df, Region == -2) %>% 
#  dplyr::select(Item, Cond, Subject, RT) %>% 
#  rename(RT_precrit2 = RT)
#df <- merge(df, df.precrit2, by = c("Item", "Cond", "Subject"))

# Add target length 
df <- df %>% mutate(Length = nchar(Target))

# Add log-transformed variables (DV + IVs)
df$logRT <- log(df$RT)
df$logRT_precrit1 <- log(df$RT_precrit1)
#df$logRT_precrit2 <- log(df$RT_precrit2)
df$logProdNorms <- log(df$ProdNorms)
df$logAssociation <- log(df$Association)
df$logInference <- log(df$Inference)
df$logCoherence <- log(df$Coherence)
#df$logInfCoPCA <- log(df$InfCoPCA)

# Order df by Region
df <- arrange(df, Region)

# Invert predictor scales 
#df$AssociationRating <- df$AssociationRating - 7
#df$CoherenceRating <- df$CoherenceRating - 7
#df$InferenceRating <- df$InferenceRating - 7

# Scale predictors (careful: needs to be performed AFTER removing data)
# Note: predictors can be scaled here instead of in the per-region dfs, since 
# predictors are identical in each region 
df <- scalePredictors(
 df,
 c(
   "ProdNorms",
   "Association",
   "Inference",
   "Coherence",
   "InfCoPCA",
   "RT_precrit1",
   #"RT_precrit2",
   "logRT_precrit1",
   #"logRT_precrit2",
   "Length"
 )
)

#str(df)
```


# Observed RTs

## Mean RTs by condition for each region of interest

**Region codes:**

- -1 = precritical
- 0 = critical (target)
- 1 = spillover 1 
- 2 = spillover 2

```{r echo = FALSE}
# Summary:
# Untransformed RTs
pander::pander(df %>%
                 group_by(Cond, Region) %>%
                 summarize(Mean = mean(RT)) %>%
                 spread(Region, Mean) %>%
                 mutate_if(is.numeric, round),
               caption = "Untransformed RTs")
# Log-transformed RTs
pander::pander(df %>%
                 group_by(Cond, Region) %>%
                 summarize(Mean = mean(logRT)) %>%
                 spread(Region, Mean),
               caption = "Log-transformed RTs")
# With SD and SE:
pander::pander(aggregMeans(df,
                  as.formula(RT ~ Region + Cond),
                  round.data = TRUE) %>%
                 mutate_if(is.numeric, round),
               caption="Untransformed RTs (long format) with SD+SE")
```

## Plots

### RTs by region and condition 

```{r echo = FALSE, fig.width=6.5, fig.height=4.5}
#df <- filter(df, !Item %in% c(5, 13, 25))
p1 <- plotSPR(df, "RT", "RT", c(260, 325))
p2 <- plotSPR(df, "logRT", "logRT", c(5.52,5.73))
ggsave("./plots/observed_RTs.pdf", p2,
       width = 3.2, height = 3.5, dpi=300)
grid.arrange(p1+ggtitle("Untransformed RTs"),
             p2+ggtitle("Log-transformed RTs"),
             ncol=2,
             top = textGrob("Observed reading times",
                            gp=gpar(fontsize=14, fontface=2)))
```


### Density plot: variation across regions 

Check the *range* of the reading times in the critical regions (i.e. the spread of the variation).

Larger ranges implies that the residuals will be larger due to the increased amount of variation in that region.

```{r echo = FALSE, warning = FALSE, fig.width=6, fig.height=4}
spread.color.palette <- c("-1" = "rosybrown1",
                          "0" = "black",
                          "1" = "deeppink2", 
                          "2" = "purple3")
# Density plot
p.spread <- df %>%
  filter(Region %in% -1:2) %>%
  ggplot(aes(
    x = logRT,
    group = Region,
    linetype = factor(Region),
    color = factor(Region)
  )) +
  geom_density(adjust = 0.8,
               linewidth = 1,
               key_glyph = draw_key_smooth) +
  #xlim(c(0, 1000)) +
  ylab("Density") +
  scale_color_manual("Region",  # Legend title
                     values = spread.color.palette,
                     labels = c("Pre-critical", "Critical",
                                "Spillover", "Post-spillover")) +
  scale_linetype_manual(
    "Region",  # Legend title
    values = c("dotdash", "solid", "dashed", "dotted"),
    labels = c("Pre-critical", "Critical",
               "Spillover", "Post-spillover")
  ) +
  theme_minimal()

ggsave("./plots/spread_RT.pdf", p.spread,
       width = 5, height = 3.5, dpi=300)

p.spread + ggtitle("Spread of RTs by Region")
```

The peaks are less pronounced for the critical and first spillover version; the critical region has the least steep rightgoing tail.


### Task effect: decreasing RTs over time

Check the presence of a task effect, i.e., decreasing RTs over the course of the experiment.

**Only critical region (target noun) RTs are plotted.**

*(Note that across lists, reversed orders counterbalanced for trial position effects!)*

**Method 1: RT ~ Block**

Aggregate mean RT ~ Block:

```{r echo = FALSE, fig.width=4, fig.height=3}
mean.RT.block <- aggregMeans(filter(df, Region==0),
                             as.formula(RT ~ Block))

p.taskeffect <- mean.RT.block %>%
  ggplot(aes(x = Block, y = Mean, color = Block)) +
  geom_point(size = 5,
             shape = "cross") +
  geom_errorbar(aes(ymin = Mean - SE,
                    ymax = Mean + SE),
                width = 0.2,
                linewidth = 0.3) +
  xlab("Time (Block)") +
  ylab("RT (ms)") +
  scale_colour_brewer(palette="Dark2")+
  theme_minimal()

ggsave("./plots/taskeffect.pdf", p.taskeffect,
       width = 4, height = 3, dpi=300)

p.taskeffect + ggtitle("Mean RT by block position")
```


**Method 2: RT ~ TrialNumber**

```{r echo = FALSE, message = FALSE, fig.width=7, fig.height=4}
mean.RT.trialnum <- df %>% 
  filter(Region == 0) %>% 
  group_by(TrialNum) %>% 
  summarize(MeanRT = mean(RT)) %>% 
  data.frame()

mean.RT.trialnum %>% 
  ggplot(aes(x=TrialNum, y=MeanRT)) + 
  geom_smooth(method = 'lm', se=TRUE, alpha = .2, color='steelblue') + 
  geom_line(color='steelblue') + 
  #ylim(c(0,350)) + 
  xlab("Time (trial position)") + 
  ylab("RT (ms)") +
  theme_minimal() +
  ggtitle("Mean RT by trial position")
```

```{r include = FALSE, fig.width=6, fig.height=4}
df %>%
  filter(Region == 0) %>% 
  ggplot(aes(x = TrialNum, y = RT, color=Subject, group=Subject)) + 
  #geom_point(size=1.5, shape="cross", alpha = 0.5) + 
  geom_smooth(method = 'lm', size = 0.5, se=TRUE, alpha = .2, aes(fill=Subject)) +
  #geom_smooth(method = 'lm', se=TRUE, alpha = .2, aes(x = TrialNum, y = RT)) +
  ggtitle("RT by Subject + trial number") +
  theme_minimal() +
  theme(legend.position = "None")
```


# Prepare models + first inspection 

## Define parameters

Define general parameters:

- regions of interest to model
- untransformed or log-transformed RTs
- based on RT unit (ms or log ms): ranges for the plot y-axis
- which kind of models to run:
  - "lm": simple LM
  - "lm.by.subj": by-subject LMs
  - "lmer": LMER

```{r}
regions <- -1:2
log.RTs <- TRUE
m.type = "lmer"

if (log.RTs == TRUE) {
  DV <- "logRT"
  precrit1 <- "logRT_precrit1"
  precrit2 <- "logRT_precrit2"
  y.unit <- "Log RT"
  y.range <- c(5.52, 5.72)
  y.range.res <- c(-0.07, 0.07)
} else {
  DV <- "RT"
  precrit1 <- "RT_precrit1"
  precrit2 <- "RT_precrit2"
  y.unit <- "RT (ms)"
  y.range <- c(260, 320)
  y.range.res <- c(-20, 20)
}
```


## Traditional analysis

```{r}
# Split data into different data frames for each region 
precrit <- filter(df, Region==-1)
crit <- filter(df, Region==0)
spill1 <- filter(df, Region==1)
spill2 <- filter(df, Region==2)

f1 <- as.formula(logRT ~ Association + InfCoPCA + logRT_precrit1 + (1|Subject) + (1|Item))
summary(lmer(f1, data=crit))
summary(lmer(f1, data=spill1))
summary(lmer(f1, data=spill2))

f2 <- as.formula(logRT ~ Association + Inference + logRT_precrit1 + (1|Subject) + (1|Item))
summary(lmer(f2, data=crit))
summary(lmer(f2, data=spill1))
summary(lmer(f2, data=spill2))

f3 <- as.formula(logRT ~ Association + Coherence + logRT_precrit1 + (1|Subject) + (1|Item))
summary(lmer(f3, data=crit))
summary(lmer(f3, data=spill1))
summary(lmer(f3, data=spill2))

f4 <- as.formula(logRT ~ Association + Coherence + logRT_precrit1 + Association*Coherence + (1|Subject) + (1|Item))
summary(lmer(f4, data=crit))
summary(lmer(f4, data=spill1))
summary(lmer(f4, data=spill2))

f5 <- as.formula(logRT ~ Association + Coherence + logRT_precrit1 + Association*Coherence + Block + (1|Subject) + (1|Item))
summary(lmer(f5, data=crit))
summary(lmer(f5, data=spill1))
summary(lmer(f5, data=spill2))

f6 <- as.formula(logRT ~ Association + Coherence + logRT_precrit1 + Association*Coherence + TrialNum + (1|Subject) + (1|Item))
summary(lmer(f6, data=crit))
summary(lmer(f6, data=spill1))
summary(lmer(f6, data=spill2))

f7 <- as.formula(logRT ~ Association + Coherence + logRT_precrit1 + Association*Coherence + TrialNum + Length + (1|Subject) + (1|Item))
summary(lmer(f7, data=crit))     # singular fit!
summary(lmer(f7, data=spill1))   # singular fit!
summary(lmer(f7, data=spill2))
```


# rRT analysis

In the following analyses, each observed RT is replaced with an estimated RT obtained from a linear mixed effects model (using the lmer function from the lme4 package, Bates et al. 2015).
Beyond analyzing the estimated RTs, this approach allows to examine how the estimates change dynamically as individual predictors are either a) incrementally added to the model (forward modeling), or b) controlled for by setting them to their grand mean value across conditions (backward modeling).

In the first part of the analysis, forward model construction is used to investigate the impact of individual predictors and perform model selection.
In particular, it allows to contrast untransformed with log-transformed versions of the same predictor, and to compare the collinear predictors Inference, Coherence and InfCoPCA (the latter being the first component resulting from a principal component analysis combining Inference and Coherence).

In the second part of the analysis, backward modeling is then used to gain insight into how the individual predictors in the full model -- corresponding to specific properties of the stimulus presented at trial $i$ -- combine at a latent level in each trial to form the resulting RT.
Concretely, the signal for a specific stimulus at each trial $i$ is broken down into a weighted sum of the individual predictors, plus the noise term $\epsilon_i$:

$$
y_i = \beta_1 x_{1i} + \beta_2 x_{2i} +  ... + \epsilon_i = \sum_{j = 1}^N \beta_j x_{ji} + \epsilon_i
$$

where 

- $\beta_j$ stands for the coefficient estimate for an individual predictor $j$;
- $x_{ji}$ stands for the stimulus property for predictor $j$ at trial $i$;
- $\epsilon_i$ stands for the noise in trial $i$

The goal of the estimation process is to find the set of $\beta_j$'s that minimizes $\epsilon_i$ across trials, given the stimulus properties $x_{ji}$.

**TODO: add lmer-adjusted formula with random effects**

The impact of each predictor in each region of interest can then be analyzed by inspecting the change in the residual error ensuing when the predictor is controlled for (i.e., set to its mean value -- which in the present analysis corresponds to setting it to zero, given that predictors were standardized to have a mean of $0$ and a standard deviation of $1$).
More precisely, an increase in residual error when controlling for a specific predictor in a model (and correspondingly a larger deviation of the estimated RTs from the observed RTs) indicates that this predictor is critical for reducing the error in that region.
This provides a quantitative measure of goodness of model fit that allows to compare different models in which the impact of the individual predictors can be isolated (where closer approximation of $\epsilon$ to $0$ means better fit).

Finally, the effect structure of the *observed RTs* and that of the *estimated rRTs* can be analyzed and compared using statistical analysis:

- First, lmer models can be run on the estimated rRTs in each region, providing estimates for the coefficients and effect size just like analysis of the observed rRTs.
- Second, an anova can be run to compare two versions of the final model, one using the observed RTs and the other using the estimated rRTs, with type as a between-subjects factor, where no significant effect of type is expected if the rRTs adequately capture the variance in the observed data.

General points:

- In the following, the different regions are analyzed as *independent models* following the hypothesis that the predictors will have different effects in different regions. 
To control for baseline offset, the precritical region is always added as a predictor in the models for the different regions (except in the baseline intercept model). 
- Note that inclusion of precritical region RT as a predictor in the model for the precritical region itself leads to perfect estimates with a singular fit warning and no remaining error, as is to be expected.
The precritical region is therefore not modelled and plotted in models that include precritical RT as a predictor. (TODO/open question: keep this way?)

## Forward modeling 

This section aims to assess the fit of individual predictors in isololation, starting from a simple baseline model containing only the intercept.

**Aims:**

A) provide preliminary insights into the variance in the signal, and the impact of each predictor in isolation
B) determine the impact of log-transformations of the predictors for the following predictors: Production Norms, Association, Inference, Coherence, InfCoPCA (and precriticalRT?)
C) compare the collinear predictors Inference, Coherence and InfCoPCA (first PCA component combining Inference+Coherence)

**Method:**

- begin with a baseline (intercept only) model, then add the individual predictors of interest one by one
- compute individual models for different vesions of a single predictor (e.g. untransformed Association vs. log-transformed Association), using a subset of the experimental conditions for which there is variation in this predictor while variation is minimized among the other predictors
- compare residual plots, AIC and BIC for the different models for each predictor

### A) 

Following the logic of Brouwer et al. (2020), the analysis starts with a baseline model of the variance in the signal. 
The baseline model assumes that no factors were manipulated and that all trials belong to the same condition. 
Therefore, the resulting estimate for each trial in a given region will simply be the average over all trials in that region, with slight offsets due to the random effects. 
Building on this baseline model, predictors are then added in isolation until we arrive at the full model including an interaction term.

**Note: In this section, I used lmer models with random intercepts for Subject and Item, but no random slopes, as the latter consistently led to convergence problems.**

```{r, results = FALSE, echo = FALSE, fig.width=9, fig.height=5}
# Run model(s) and generate plots
m.formula <- as.formula(paste0(DV, " ~ 1 + (1|Subject) + (1|Item)"))
modelAndPlot("Intercept",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = FALSE)

#TODO: 
# - remove top title for saved plot
# - make legend larger but keep only 1 legend per grid.arrange
# - begin from building the full model and then eliminate predictors
```

Next, the logRTs on the precritical region are included as a predictor in the baseline model to account for the baseline offset observed in the precritical region.

The pre-critical region will therefore no longer be modeled and plotted in the remainder of this section, since its RTs are used as a predictor, always leading to perfect fit in the precritical region.

For the other regions, the residual plot shows that including precritical RT alone as a predictor does not notably improve model fit:

```{r, results = FALSE, echo = FALSE, fig.width=9, fig.height=5}
# change the default regions: remove precritical from models 
regions <- 0:2
m.formula <- as.formula(paste0(DV, " ~ 1 + logRT_precrit1 + (1|Subject) + (1|Item)"))
modelAndPlot("Intercept + Precritical RT",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = FALSE)
```

In a next step, Association is added as a further predictor to the model, again not leading to drastic improvements in fit:

```{r, results = FALSE, echo = FALSE, fig.width=9, fig.height=5}
m.formula <- as.formula(paste0(DV, " ~ 1 + logRT_precrit1 + Association + (1|Subject) + (1|Item)"))
modelAndPlot("Intercept + RT_precrit + Association",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = FALSE)
```

The next three plots add Coherence, Inference and InfCoPCA individually to the baseline model (Intercept + precritical RT), all three leading to improvements in model fit:

```{r, results = FALSE, echo = FALSE, fig.width=9, fig.height=5}
m.formula <- as.formula(paste0(DV, " ~ 1 +  logRT_precrit1 + Coherence + (1|Subject) + (1|Item)"))
modelAndPlot("Intercept + RT_precrit + Coherence",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = FALSE)
```

```{r, results = FALSE, echo = FALSE, fig.width=9, fig.height=5}
m.formula <- as.formula(paste0(DV, " ~ 1 +  logRT_precrit1 + Inference + (1|Subject) + (1|Item)"))
modelAndPlot("Intercept + RT_precrit + Inference",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = FALSE)
```

```{r, results = FALSE, echo = FALSE, fig.width=9, fig.height=5}
m.formula <- as.formula(paste0(DV, " ~ 1 +  logRT_precrit1 + InfCoPCA + (1|Subject) + (1|Item)"))
modelAndPlot("Intercept + RT_precrit + InfCoPCA",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = FALSE)
```

Next, Association + Coherence together are added to the model, first without an interaction term between them...

```{r, results = FALSE, echo = FALSE, fig.width=9, fig.height=5}
m.formula <- as.formula(paste0(DV, " ~ 1 + Association + Coherence + 
   logRT_precrit1 + (1 | Subject) + (1 | Item)"))
modelAndPlot("Full model (no interaction)",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = FALSE)
```

...and then with an interaction term between Association and Coherence, leading again to improved model fit (this is the full model):

```{r, results = FALSE, echo = FALSE, fig.width=9, fig.height=5}
m.formula <- as.formula(paste0(DV, " ~ 1 + Association + Coherence + 
   logRT_precrit1 +  Association*Coherence + (1|Subject) + (1|Item)"))
modelAndPlot("Full model",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = FALSE)
```

Next, we can also check if it helps to include Block, TrialNumber and/or Target noun length as predictors.
However, as these factors were counterbalanced across lists / conditions, they are not hypothesized to lead to improvements in model fit.

```{r results = FALSE, echo = FALSE}
m.formula <- as.formula(paste0(DV, " ~ 1 + Association + Coherence + 
   logRT_precrit1 + Block +
   Association*Coherence + (1 | Subject) + (1 | Item)"))
modelAndPlot("Full model with Block position as predictor",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = FALSE)

m.formula <- as.formula(paste0(DV, " ~ 1 + Association + Coherence + 
   logRT_precrit1 + TrialNum +
   Association*Coherence + (1 | Subject) + (1 | Item)"))
modelAndPlot("Full model with Trial Number as predictor",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = FALSE)

m.formula <- as.formula(logRT ~ Association + Coherence + logRT_precrit1 + Association*Coherence + TrialNum + Length + (1|Subject) + (1|Item))
modelAndPlot("Full model with TrialNum and Target Length",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = FALSE)
```


### B) Modeling subsets of the data

We now turn to assessing the impact of individual predictors and transformations thereof in subsets of the full data.
More specifically, ...

Use conditions C and D for Association, otherwise use all conditions.

In the code below, replace "Association" and "logAssociation" by the respective variables to compare.

```{r, results = FALSE, echo = FALSE, fig.width=7, fig.height=4}
df.AB <- filter(df, Cond %in% c("A", "B"))
df.CD <- filter(df, Cond %in% c("C", "D"))

#TODO: allow for df.CD with respective colors
#compareLogToUntransformed(df.AB, "ProdNorms", "logProdNorms")
compareLogToUntransformed(df.AB, "Association", "logAssociation")
compareLogToUntransformed(df.AB, "RT_precrit1", "logRT_precrit1")
compareLogToUntransformed(df.AB, "Inference", "logInference")
compareLogToUntransformed(df.AB, "Coherence", "logCoherence")
compareLogToUntransformed(df.AB, "RT_precrit1", "logRT_precrit1")
```

**Results:** Comparing the residuals and error bars reveals no differences between the predictors and their log-transformed counterparts. 
This was tested for: 

- Association vs. logAssociation
- Inference vs. logInference
- Coherence vs. logCoherence
- RT_precrit1 vs. logRT_precrit1

Therefore, the original (scaled) versions of the predictors are kept, except for logRT_precrit1, which is used since the DV is also log-transformed ms.

**B) Compare the collinear predictors Inference, Coherence and InfCoPCA**

Use residual plots and model comparison measures (AIC and BIC) to compare the collinear predictors.
Residuals: 

```{r, fig.width=10, fig.height=4}
f1 <- as.formula(get(DV) ~ 1 + Inference + (1|Subject) + (1|Item))
df.m1 <- runModels(df, model.type="lmer", f1)
p1 <- plotSPR(df.m1, "Residuals", y.unit, y.range.res) + ggtitle("Residuals: Inference")
#ggsave("./plots/residuals_Inference.pdf", last_plot(), width=3.5, height=3)

f2 <- as.formula(get(DV) ~ 1 + Coherence + (1|Subject) + (1|Item))
df.m2 <- runModels(df, model.type="lmer", f2)
p2 <- plotSPR(df.m2, "Residuals", y.unit, y.range.res) + ggtitle("Residuals: Coherence")

f3 <- as.formula(get(DV) ~ 1 + InfCoPCA + (1|Subject) + (1|Item))
df.m3 <- runModels(df, model.type="lmer", f3)
p3 <- plotSPR(df.m3, "Residuals", y.unit, y.range.res) + ggtitle("Residuals: InfCoPCA")

p <- grid.arrange(p1, p2, p3, nrow=1)
ggsave("./plots/residuals_Inf_Co_InfCoPCA.pdf", p, width=10, height=4)
```

AIC and BIC (f1 = Inference, f2 = Coherence, f3 = InfCoPCA):

(Use only the critical and spillover region, exclude precritical.)


```{r}
# AIC scores
getMeanAICBIC(df, f1, "AIC", regions=0:2)
getMeanAICBIC(df, f2, "AIC", regions=0:2)
getMeanAICBIC(df, f3, "AIC", regions=0:2)
# BIC scores
getMeanAICBIC(df, f1, "BIC", regions=0:2)
getMeanAICBIC(df, f2, "BIC", regions=0:2)
getMeanAICBIC(df, f3, "BIC", regions=0:2)
```

**Result:** both mean AIC and mean BIC across regions is lowest for Coherence.

AIC and BIC for combined full models:

```{r}
f1 <- as.formula(get(DV) ~ 1 + Association + Inference + Association*Inference + logRT_precrit1 + (1|Subject) + (1|Item))
f2 <- as.formula(get(DV) ~ 1 + Association + Coherence + Association*Coherence + logRT_precrit1 + (1|Subject) + (1|Item))
f3 <- as.formula(get(DV) ~ 1 + Association + InfCoPCA + Association*InfCoPCA + logRT_precrit1 + (1|Subject) + (1|Item))

# AIC scores
getMeanAICBIC(df, f1, "AIC", regions=0:2)
getMeanAICBIC(df, f2, "AIC", regions=0:2)
getMeanAICBIC(df, f3, "AIC", regions=0:2)
# BIC scores
getMeanAICBIC(df, f1, "BIC", regions=0:2)
getMeanAICBIC(df, f2, "BIC", regions=0:2)
getMeanAICBIC(df, f3, "BIC", regions=0:2)
```


```{r}
knitr::knit_exit()
```


## Setting predictors to zero

Improved version: 

```{r, fig.height = 7, fig.width = 14}
m.formula <- as.formula(paste0(
  DV,
  " ~ Association + Coherence + Association:Coherence + ",
  precrit1
))

m <- runModels(df,
               "lm.by.subj",
               m.formula,
               regions)

# Set predictors to zero: start with all zero except the intercept,
# then add in more predictors

# Plot only the intercept
m$InterceptOnly <- m$`Coef_(Intercept)`
plotSPR(m, "InterceptOnly", "Estimates: Intercept only", y.unit, y.range)

# Add association only
m$InterceptAssoc <- m$`Coef_(Intercept)` + m$Association*m$Coef_Association 
plotSPR(m, "InterceptAssoc", "Estimates: Intercept+Assoc", y.unit, y.range)

# Add coherence only
m$InterceptCoherence <- m$`Coef_(Intercept)` + m$Coherence*m$Coef_Coherence
plotSPR(m, "InterceptCoherence", "Estimates: Intercept+Coherence", y.unit, y.range)

# Add precritical only 
m$InterceptPrecrit1 <- m$`Coef_(Intercept)` + m$logRT_precrit1*m$Coef_logRT_precrit1
plotSPR(m, "InterceptPrecrit1", "Estimates: Intercept+Precrit1", y.unit, y.range)

# Add everything except association 
m$InterceptCoherencePrecrit1Interaction <- m$`Coef_(Intercept)` + m$Association*0 + m$Coherence*m$Coef_Coherence + m$logRT_precrit1*m$Coef_logRT_precrit1 + m$Association*m$Coherence*m$`Coef_Association:Coherence`
p1 <- plotSPR(m, "InterceptCoherencePrecrit1Interaction", "Estimates: Full model without Association",
        y.unit, y.range)

# Add everything except coherence 
m$InterceptAssocPrecrit1Interaction <- m$`Coef_(Intercept)` + m$Association*m$Coef_Association + m$Coherence*0 + m$logRT_precrit1*m$Coef_logRT_precrit1 + m$Association*m$Coherence*m$`Coef_Association:Coherence`
p2 <- plotSPR(m, "InterceptAssocPrecrit1Interaction", "Estimates: Full model without Coherence",
        y.unit, y.range)

# Add everything except precritical 1 
m$InterceptAssocCoherenceInteraction <- m$`Coef_(Intercept)` + m$Association*m$Coef_Association + m$Coherence*m$Coef_Coherence + m$logRT_precrit1*0 + m$Association*m$Coherence*m$`Coef_Association:Coherence`
p3 <- plotSPR(m, "InterceptAssocCoherenceInteraction", "Estimates: Full model without Precrit1",
        y.unit, y.range)


# Add everything except the interaction term 
m$InterceptAssocCoherencePrecrit1 <- m$`Coef_(Intercept)` + m$Association*m$Coef_Association + m$Coherence*m$Coef_Coherence + m$logRT_precrit1*m$Coef_logRT_precrit1 + m$Association*m$Coherence*0
p4 <- plotSPR(m, "InterceptAssocCoherencePrecrit1", "Estimates: Full model without the interaction term",
        y.unit, y.range)

# Sanity check: full model 
m$Full <- m$`Coef_(Intercept)` + m$Association*m$Coef_Association  + m$Coherence*m$Coef_Coherence + m$logRT_precrit1*m$Coef_logRT_precrit1 +  m$Association*m$Coherence*m$`Coef_Association:Coherence`
p5 <- plotSPR(m, "Full", "Estimates: Full model", y.unit, y.range)

# Observed data 
p.observed <- plotSPR(m, DV, "Observed RTs", y.unit, y.range)

grid.arrange(p1, p2, p3, 
             p4, p5, p.observed,
             nrow=2,
             top = textGrob("Setting individual predictors to zero", gp=gpar(fontsize=14, fontface=2)))
```
