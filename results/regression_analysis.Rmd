---
title: "SPR results: regression-based analysis of the reading times"
author: "Miriam Schulz"
date: "10/11/2022"
output:
  html_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      collapse = TRUE,
                      fig.height = 6, fig.width = 9)
```


# About

This script contains an rRT analysis of the inference reading time data (SPR1) based on the rERP approach in Brouwer, Delogu & Crocker (2020). 


# Read data and preprocess

In this section:

- read in the data
- remove outliers based on 
    - SPR (word-by-word) RTs
    - context sentence RTs
- remove incorrectly answered trials (plausibility ratings)
- add log-transformed RTs
- standardize predictors

```{r include = FALSE}
# Workspace and libraries
rm(list = ls())
library(tidyverse)
library(lme4)              # for lmer()
library(lmerTest)          # for summary(lmer.model)
library(broom)             # for tidy(modeloutput)
library(gridExtra)         # for grid.arrange()
library(grid)              # for textGrob()
library(RColorBrewer)      # for color palettes
library(pander)            # for pander(df)
source("functions.R")      # custom preprocessing and helper functions
source("functions_rRT.R")  # rRT functions
options(dplyr.summarise.inform = FALSE)  # remove message in output
```

Read in data and remove fillers, practice and non-critical regions:

```{r message=FALSE}
# Read data
df <- read.csv("results_reads.csv", header = TRUE)

# Exclude filler and practice trials
df <- removeFillersPractice(df)

# Exclude non-critical regions
df <- removeRegions(df, keep.regions=-1:2)
```

Next, the raw data are visualized with histograms and QQ plots before removing outliers.

Histograms and residual plots:

```{r echo=FALSE, fig.width=10, fig.height=10}
par(mfrow=c(2,2))
# Histograms

# Histogram of RTs:
hist(df$RT,
     breaks = 50,
     col='steelblue',
     xlab = "RTs (ms)",
     main='Untransformed RTs')

# Histogram of log RTs :
hist(log(df$RT),
     breaks = 50,
     col='steelblue',
     xlab = "logRTs (ms)",
     main='Log-transformed RTs')

# QQ plots

# Untransformed RTs
qqnorm(df$RT,
       col='steelblue',
       pch=16,
       main='QQ plot of untransformed RTs')

# Log-transformed RTs
qqnorm(log(df$RT),
       col='steelblue',
       pch=16,
       main='QQ plot of log-transformed RTs')
# Save plot
ggsave("./plots/RT_hist_QQ_preDataExclusion.pdf", plot = last_plot(),
       width = 10, height = 10, dpi = 300)
par(mfrow=c(1,1))  # reset
```

## Remove SPR outliers

```{r message=FALSE}
# Optional: remove items with low accuracy in B
#df <- filter(df, ! Item %in% c(5,13,25))

# Remove outliers: SPR (word RTs) - SD
df <- removeOutliersSPR(df,
                        method = "sd",
                        sd.value = 4,
                         entire.trial = TRUE,
                        regions = -1:2)
```

Histograms and residual plots *after* removing SPR outliers:

```{r echo=FALSE, fig.width=10, fig.height=10}
par(mfrow=c(2,2))
# Histograms

# Histogram of RTs:
hist(df$RT,
     breaks = 50,
     col='steelblue',
     xlab = "RTs (ms)",
    main='Untransformed RTs')

# Histogram of log RTs :
hist(log(df$RT),
     breaks = 50,
     col='steelblue',
     xlab = "logRTs (ms)",
     main='Log-transformed RTs')

# QQ plots

# Untransformed RTs
qqnorm(df$RT,
       col='steelblue',
       pch=16,
       main='QQ plot of untransformed RTs')

# Log-transformed RTs
qqnorm(log(df$RT),
       col='steelblue',
       pch=16,
       main='QQ plot of log-transformed RTs')
# Save plot
ggsave("./plots/RT_hist_QQ_postDataExclusion.pdf", plot = last_plot(),
       width = 10, height = 10, dpi = 300)
par(mfrow=c(1,1))  # reset
```

Box-Cox plot:

```{r echo=FALSE, fig.width=4, fig.height=4}
# Boxcox plot
MASS::boxcox(df$RT ~ df$Cond)
```

Normality test (Shapiro-Wilk):

```{r}
# Untransformed RTs:
shapiro.test(filter(df, Region >= 0)$RT)

# Log-transformed RTs:
shapiro.test(log(filter(df, Region >= 0)$RT))
```

## Remove context sentence RT outliers and incorrectly answered trials

```{r fig.width=10, fig.height=5}
# Remove outliers: context sentence reading times
df <- removeOutliersContext(df, min.rt=500, max.rt=30000)

# Remove trials with incorrect plausibility ratings
df <- removeIncorrect(df)

# Visualize context sentence reading times with a histogram:
par(mfrow=(c(1,2)))
hist(
    filter(df, Region == 0)$ContextRT,
    breaks = 50,
    main = "Untransformed Context Sentence RTs",
    xlab = "Context RTs (ms)",
    col = "steelblue"
    # main = NULL
)
hist(
    log(filter(df, Region == 0)$ContextRT),
    breaks = 50,
    main = "Log-transformed Context Sentence RTs",
    xlab = "Context RTs (log-ms)",
    col = "steelblue"
)
par(mfrow=(c(1,1))) # reset
```

```{r echo=FALSE, fig.width=4, fig.height=3.5}
# Visualize average context reading times by condition:
cond.means.contextRTs <- aggregMeans(df, as.formula(ContextRT ~ Cond))
round(cond.means.contextRTs$Mean)
round(cond.means.contextRTs$SD)

p <-
  cond.means.contextRTs %>% 
  ggplot(aes(x = Cond, y = Mean, fill=Cond)) + 
  geom_bar(stat='identity', width=0.8) + 
  geom_errorbar(
    aes(ymin = Mean - SE, ymax = Mean + SE),
    width = 0.15,
    linewidth = 0.3
  ) +
  ylim(0, 3000) +
  xlab("Condition") +
  ylab("RT (ms)") +
  scale_fill_manual(values = c("cornflowerblue", "chartreuse3",
                               "tomato2", "darkgoldenrod1")) +
  theme_minimal() +
  theme(legend.position="None",
        text = element_text(size = 13))
filename <- paste0("./plots/", "context_RT.pdf")
ggsave(filename, plot = p, width = 3, height = 3)
p + ggtitle("Context sentence RT by condition")
```

## Further preprocessing: transformations etc.

In this subsection: 

- Rename variables
- Transform to factor when necessary
- Combine Inference+Coherence ratings into a new combined predictor with PCA, extracting the first component
- add precritical reading times as predictors 
- add log-transformed versions of the reading times (the SPR RT (DV) as well as predictors (IVs))
- standardize the predictors

```{r message=FALSE}
# Rename variables 
rating_colnames <- str_remove(names(df), "Rating")
names(df) <- rating_colnames
df <- rename(df, ProdNorms = ProductionPercentage)

# Transform variables to factors
df$Cond <- as.factor(df$Cond)
df$Item <- as.factor(as.numeric(as.character(df$Item)))
df$Subject <- as.factor(as.numeric(as.character(df$Subject)))

# Transform the binary plausibility predictor to numeric, then to factor
df$Plausible <- as.factor(ifelse(df$Plausible == "Yes", 1, 0))

# Better: combine the predictors using PCA
pca_fit <- stats::prcomp(dplyr::select(df, Inference, Coherence))#, center=FALSE, scale=FALSE)
df$InfCoPCA <- pca_fit$x[,1]
#TODO improve?: add abs(min value) + 0.01 for smoothing before log-transformation (result: vector > 0)
df$InfCoPCA <- df$InfCoPCA + abs(min(df$InfCoPCA)) + 0.01

# Extract precritical RT -1 and add to df:
df.precrit1 <- filter(df, Region == -1) %>% 
  dplyr::select(Item, Cond, Subject, RT) %>% 
  rename(RT_precrit1 = RT)
df <- merge(df, df.precrit1, by = c("Item", "Cond", "Subject"))

# Add target length 
df <- df %>% mutate(Length = nchar(Target))

# Add log-transformed variables (DV + IVs)
df$logRT <- log(df$RT)
df$logRT_precrit1 <- log(df$RT_precrit1)
df$logProdNorms <- ifelse(df$ProdNorms > 0,
                          log(df$ProdNorms), 0.01)  # smoothe for 0
df$logAssociation <- log(df$Association)
df$logInference <- log(df$Inference)
df$logCoherence <- log(df$Coherence)
df$logInfCoPCA <- log(df$InfCoPCA)

# Add interaction variables
df$AssocInference <- df$Association*df$Inference
df$AssocCoherence <- df$Association*df$Coherence

# Order df by Region
df <- arrange(df, Region)

# Invert predictor scales 
#df$AssociationRating <- 7 - df$AssociationRating
#df$CoherenceRating <- 7 - df$CoherenceRating
#df$InferenceRating <- 7 - df$InferenceRating

# Scale predictors (careful: needs to be performed AFTER removing data)
df <- scalePredictors(
  df,
  c(
    "ProdNorms",
    "Association",
    "Inference",
    "Coherence",
    "AssocInference",
    "AssocCoherence",
    "InfCoPCA",
    "RT_precrit1",
    "logRT_precrit1",
    #"RT_precrit2",
    #"logRT_precrit2",
    "Length",
    "logProdNorms",
    "logAssociation",
    "logInference",
    "logCoherence",
    "logInfCoPCA"
  )
)

#str(df)
```


# Observed RTs

## Mean RTs by condition for each region of interest

```{r echo = FALSE}
# Summary:
# Untransformed RTs
pander::pander(df %>%
                 group_by(Cond, Region) %>%
                 summarize(Mean = mean(RT)) %>%
                 spread(Region, Mean) %>%
                 mutate_if(is.numeric, round) %>% 
                 rename("precritical" = "-1",
                        "critical" = "0", 
                        "spillover1" = "1", 
                        "spillover2" = "2"),
               caption = "Untransformed RTs")
# Log-transformed RTs
pander::pander(df %>%
                 group_by(Cond, Region) %>%
                 summarize(Mean = mean(logRT)) %>%
                 spread(Region, Mean) %>% 
                 rename("precritical" = "-1",
                        "critical" = "0", 
                        "spillover1" = "1", 
                        "spillover2" = "2"),
               caption = "Log-transformed RTs")
# With SD and SE:
#pander::pander(aggregMeans(df,
#                  as.formula(RT ~ Region + Cond),
#                  round.data = TRUE) %>%
#                 mutate_if(is.numeric, round),
#               caption="Untransformed RTs (long format) with SD+SE")
```

## Plots

### RTs by region and condition 

```{r echo = FALSE, fig.width=6.5, fig.height=4.5}
#df <- filter(df, !Item %in% c(5, 13, 25))
p1 <- plotSPR(df, "RT", "RT", c(260, 325))
p2 <- plotSPR(df, "logRT", "logRT", c(5.52,5.73))
ggsave("./plots/observed_RTs.pdf", p2,
       width = 3.2, height = 3.5, dpi=300)
grid.arrange(p1+ggtitle("Untransformed RTs"),
             p2+ggtitle("Log-transformed RTs"),
             ncol=2,
             top = textGrob("Observed reading times",
                            gp=gpar(fontsize=14, fontface=2)))
```


### Density plot: spread in each region

Check the *range* of the reading times in the critical regions (i.e. the spread of the variation).

Larger ranges implies that the residuals will be larger due to the increased amount of variation in that region.

```{r echo = FALSE, warning = FALSE, fig.width=6, fig.height=4}
spread.color.palette <- c("-1" = "rosybrown1",
                          "0" = "black",
                          "1" = "deeppink2", 
                          "2" = "purple3")
# Density plot
p.spread <- df %>%
  filter(Region %in% -1:2) %>%
  ggplot(aes(
    x = logRT,
    group = Region,
    linetype = factor(Region),
    color = factor(Region)
  )) +
  geom_density(adjust = 0.8,
               linewidth = 1,
               key_glyph = draw_key_smooth) +
  #xlim(c(0, 1000)) +
  ylab("Density") +
  scale_color_manual("Region",  # Legend title
                     values = spread.color.palette,
                     labels = c("Pre-critical", "Critical",
                                "Spillover", "Post-spillover")) +
  scale_linetype_manual(
    "Region",  # Legend title
    values = c("dotdash", "solid", "dashed", "dotted"),
    labels = c("Pre-critical", "Critical",
               "Spillover", "Post-spillover")
  ) +
  theme_minimal()

ggsave("./plots/spread_RT.pdf", p.spread,
       width = 5, height = 3.5, dpi=300)

p.spread + ggtitle("Spread of RTs by Region")
```

The peaks are less pronounced for the critical and first spillover version; the critical region has the least steep rightgoing tail.


### Task effect: decreasing RTs over time

Check the presence of a task effect, i.e., decreasing RTs over the course of the experiment.

**Only critical region (target noun) RTs are plotted.**

*(Note that across items, reversed list orders counterbalanced for trial position effects!)*

**Method 1: RT ~ Block**

```{r echo = FALSE, fig.width=4, fig.height=3}
mean.RT.block <- aggregMeans(filter(df, Region==0),
                             as.formula(RT ~ Block))

p.taskeffect <- mean.RT.block %>%
  ggplot(aes(x = Block, y = Mean, color = Block)) +
  geom_point(size = 5,
             shape = "cross") +
  geom_errorbar(aes(ymin = Mean - SE,
                    ymax = Mean + SE),
                width = 0.2,
                linewidth = 0.3) +
  xlab("Time (Block)") +
  ylab("RT (ms)") +
  scale_colour_brewer(palette="Dark2")+
  theme_minimal()

ggsave("./plots/taskeffect.pdf", p.taskeffect,
       width = 4, height = 3, dpi=300)

p.taskeffect + ggtitle("Mean RT by block position")
```


**Method 2: RT ~ TrialNumber**

```{r echo = FALSE, message = FALSE, fig.width=7, fig.height=4}
mean.RT.trialnum <- df %>% 
  filter(Region == 0) %>% 
  group_by(TrialNum) %>% 
  summarize(MeanRT = mean(RT)) %>% 
  data.frame()

mean.RT.trialnum %>% 
  ggplot(aes(x=TrialNum, y=MeanRT)) + 
  geom_smooth(method = 'lm', se=TRUE, alpha = .2, color='steelblue') + 
  geom_line(color='steelblue') + 
  #ylim(c(0,350)) + 
  xlab("Time (trial position)") + 
  ylab("RT (ms)") +
  theme_minimal() +
  ggtitle("Mean RT by trial position")
```

```{r include = FALSE, fig.width=6, fig.height=4}
df %>%
  filter(Region == 0) %>% 
  ggplot(aes(x = TrialNum, y = RT, color=Subject, group=Subject)) + 
  #geom_point(size=1.5, shape="cross", alpha = 0.5) + 
  geom_smooth(method = 'lm', size = 0.5, se=TRUE, alpha = .2, aes(fill=Subject)) +
  #geom_smooth(method = 'lm', se=TRUE, alpha = .2, aes(x = TrialNum, y = RT)) +
  ggtitle("RT by Subject + trial number") +
  theme_minimal() +
  theme(legend.position = "None")
```


# Traditional analysis

Run a separate lmer model for each region and observe p-values.

(The precritical region is not modeled, since precritical RT is used as a predictor.)

**TODO: use contrast-coded Condition as predictor instead of continuous predictors for the "traditional" analysis?**

```{r}
# Split data into different data frames for each region 
contrasts(df$Cond) <- contr.sum(4)
contrasts(df$Cond) <- contr.treatment(4)
contrasts(df$Cond)
df$Cond <- relevel(df$Cond, ref="A")
contrasts(df$Cond)

crit <- filter(df, Region==0)
spill1 <- filter(df, Region==1)
spill2 <- filter(df, Region==2)

f <- as.formula(logRT ~ Cond + logRT_precrit1 + (1|Subject) + (1|Item))

#######################
### CRITICAL REGION ###
#######################

summary(lmer(f, data=crit))

##############################
### FIRST SPILLOVER REGION ### 
##############################

summary(lmer(f, data=spill1))

###############################
### SECOND SPILLOVER REGION ### 
###############################

summary(lmer(f, data=spill2))
```

**Observations:**

- Only precritical RT is significant in the target region, suggesting that the remaining predictors cannot capture the variance in that region
- Coherence becomes significant in the first spillover region
- Coherence and Association are significant in the second spillover region 
- The interaction Association:Coherence is not significant in any of the regions

```{r include = FALSE}
# Compare this model to a model without the interaction term:
f1 <- as.formula(logRT ~ Association * Coherence + (1|Subject) + (1|Item))
f2 <- as.formula(logRT ~ Association + Coherence + (1|Subject) + (1|Item))
# AIC
getMeanAICBIC(df, f1, "AIC", regions=0:2)
getMeanAICBIC(df, f2, "AIC", regions=0:2)
# BIC
getMeanAICBIC(df, f1, "BIC", regions=0:2)
getMeanAICBIC(df, f2, "BIC", regions=0:2)
```


```{r include = FALSE}
f1 <- as.formula(logRT ~ Association + InfCoPCA + logRT_precrit1 + (1|Subject) + (1|Item))
summary(lmer(f1, data=crit))
summary(lmer(f1, data=spill1))
summary(lmer(f1, data=spill2))

f2 <- as.formula(logRT ~ Association + Inference + logRT_precrit1 + (1|Subject) + (1|Item))
summary(lmer(f2, data=crit))
summary(lmer(f2, data=spill1))
summary(lmer(f2, data=spill2))

f3 <- as.formula(logRT ~ Association + Coherence + logRT_precrit1 + (1|Subject) + (1|Item))
summary(lmer(f3, data=crit))
summary(lmer(f3, data=spill1))
summary(lmer(f3, data=spill2))

f4 <- as.formula(logRT ~ Association + Coherence + logRT_precrit1 + Association*Coherence + (1|Subject) + (1|Item))
summary(lmer(f4, data=crit))
summary(lmer(f4, data=spill1))
summary(lmer(f4, data=spill2))

f5 <- as.formula(logRT ~ Association + Coherence + logRT_precrit1 + Association*Coherence + Block + (1|Subject) + (1|Item))
summary(lmer(f5, data=crit))
summary(lmer(f5, data=spill1))
summary(lmer(f5, data=spill2))

f6 <- as.formula(logRT ~ Association + Coherence + logRT_precrit1 + Association*Coherence + TrialNum + (1|Subject) + (1|Item))
summary(lmer(f6, data=crit))
summary(lmer(f6, data=spill1))
summary(lmer(f6, data=spill2))

f7 <- as.formula(logRT ~ Association + Coherence + logRT_precrit1 + Association*Coherence + TrialNum + Length + (1|Subject) + (1|Item))
summary(lmer(f7, data=crit))     # singular fit!
summary(lmer(f7, data=spill1))   # singular fit!
summary(lmer(f7, data=spill2))
```


# rRT analysis

In the following analyses, each observed RT is replaced with an estimated RT obtained from a linear mixed effects model (using the lmer function from the lme4 package, Bates et al. 2015).
Beyond analyzing the estimated RTs, this approach allows to examine how the estimates change dynamically as individual predictors are either a) incrementally added to the model **(forward model construction)**, or b) controlled for by setting them to their grand mean value across conditions, which lets us inspect the contribution of individual predictors to goodness of fit on a by-condition and by-region basis.

In the first part of the analysis, **forward model construction** is used to investigate the impact of individual predictors and perform model selection.
In particular, this approach allows to contrast untransformed with log-transformed versions of the same predictor, and to compare the collinear predictors Inference, Coherence and InfCoPCA (the latter being the first component resulting from a principal component analysis combining Inference and Coherence).

In the second part of the analysis, a technique based on neutralizing predictors in the full model is then used to gain insight into how the individual predictors -- corresponding to specific properties of the stimulus presented at trial $i$ -- combine at a latent level in each trial to form the resulting RT.
Concretely, the signal for a specific stimulus at each trial $i$ is broken down into a weighted sum of the individual predictors, plus the noise term $\epsilon_i$:

$$
\begin{align}
y_i &=  \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} +  ... + \epsilon_i \\
& = \beta_0 + \sum_{j = 1}^N \beta_j x_{ji} + \epsilon_i
\end{align}
$$

where 

- $\beta_0$ is the intercept;
- $\beta_j$ stands for the coefficient estimate for an individual predictor $j$;
- $x_{ji}$ stands for the stimulus property for predictor $j$ at trial $i$;
- $\epsilon_i$ stands for the noise in trial $i$

The goal of the estimation process is to find the set of $\beta_j$'s that minimizes $\epsilon_i$ across trials, given the stimulus properties $x_{ji}$.

**TODO: add lmer-adjusted formula with random effects**

The impact of each predictor in each region of interest can then be analyzed by inspecting the change in the residual error ensuing when the predictor is controlled for (i.e., when its coefficient is set to its mean value -- which in the present analysis corresponds to setting it to zero, given that predictors were standardized to have a mean of $0$ and a standard deviation of $1$).
More precisely, an increase in residual error when controlling for a specific predictor in a model (and correspondingly a larger deviation of the estimated RTs from the observed RTs) indicates that this predictor is critical for reducing the error in that region.
This provides a quantitative measure of goodness of model fit that allows to compare different models in which the impact of the individual predictors can be isolated (where closer approximation of $\epsilon$ to $0$ means better fit).

Finally, the **effect structure** of the *observed RTs* and that of the *estimated rRTs* can be analyzed and compared using statistical analysis:

- First, lmer models can be run on the estimated rRTs in each region, providing estimates for the coefficients and effect size just like analysis of the observed rRTs.
- Second, an anova can be run to compare two versions of the final model, one using the observed RTs and the other using the estimated rRTs, with type as a between-subjects factor, where no significant effect of type is expected if the rRTs adequately capture the variance in the observed data.

General points:

- In the following, the different regions are analyzed as *independent models* following the hypothesis that the predictors will have different effects in different regions. 
To control for baseline offset, the precritical region is always added as a predictor in the models for the different regions (except in the baseline intercept model). 
- Note that inclusion of precritical region RT as a predictor in the model for the precritical region itself leads to perfect estimates with a singular fit warning and no remaining error, as is to be expected.
The precritical region is therefore not modelled and plotted in models that include precritical RT as a predictor. (TODO/open question: keep this way?)


## Define parameters

Define general parameters:

- regions of interest to model
- untransformed or log-transformed RTs
- based on RT unit (ms or log ms): ranges for the plot y-axis
- which kind of models to run:
  - "lm": simple LM
  - "lm.by.subj": by-subject LMs
  - "lmer": LMER

```{r}
regions <- -1:2
log.RTs <- TRUE
m.type = "lmer"

if (log.RTs == TRUE) {
  DV <- "logRT"
  precrit1 <- "logRT_precrit1"
  precrit2 <- "logRT_precrit2"
  y.unit <- "Log RT"
  y.range <- c(5.52, 5.72)
  y.range.res <- c(-0.07, 0.07)
} else {
  DV <- "RT"
  precrit1 <- "RT_precrit1"
  precrit2 <- "RT_precrit2"
  y.unit <- "RT (ms)"
  y.range <- c(260, 320)
  y.range.res <- c(-20, 20)
}
```

**For the final analyses, lmer models with log-transformed RTs as the DV were used.**


## Forward model construction

This section aims to assess the fit of individual predictors in isololation, starting from a simple baseline model containing only the intercept.

**Aims:**

A) provide preliminary insights into the variance in the signal, and the impact of each predictor in isolation
B) determine the impact of log-transformations of the predictors for the following predictors: Production Norms, Association, Inference, Coherence, InfCoPCA (and precriticalRT?)
C) compare the collinear predictors Inference, Coherence and InfCoPCA (first PCA component combining Inference+Coherence)

**Method:**

A) begin with a baseline (intercept only) model, then add the individual predictors of interest one by one
B) compare residual plots for different versions of the *same* predictor (e.g. untransformed Association vs. log-transformed Association), using a subset of the experimental conditions for which there is variation in this predictor while variation is minimized among the other predictors
C) compare residual plots for different correlated predictors: Inference, Coherence, InfCoPCA

### A) From baseline to full model

Following the logic of Brouwer et al. (2020), the analysis starts with a baseline model of the variance in the signal. 
The baseline model assumes that no factors were manipulated and that all trials belong to the same condition. 
Therefore, the resulting estimate for each trial in a given region will simply be the average over all trials in that region, with slight offsets due to the random effects. 
Building on this baseline model, predictors are then added in isolation until we arrive at the full model including an interaction term.

**Note: In this section, I used lmer models with random intercepts for Subject and Item, but no random slopes, as the latter consistently led to convergence problems. This could be due to data sparsity.**

```{r, results = FALSE, echo = FALSE, fig.width=9, fig.height=5}
# Run model(s) and generate plots
m.formula <- as.formula(paste0(DV, " ~ 1 + (1|Subject) + (1|Item)"))
modelAndPlot("Intercept",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = FALSE)

#TODO: 
# - remove top title for saved plot
# - make legend larger but keep only 1 legend per grid.arrange
# - begin from building the full model and then eliminate predictors
```

Next, the logRTs on the precritical region are included as a predictor in the baseline model to account for the baseline offset observed in the precritical region.

The pre-critical region will therefore no longer be modeled and plotted in the remainder of this section, since its RTs are used as a predictor, always leading to perfect fit in the precritical region.

For the other regions, the residual plot shows that including precritical RT alone as a predictor does not notably improve model fit:

```{r, results = FALSE, echo = FALSE, fig.width=9, fig.height=5}
# change the default regions: remove precritical from models 
regions <- 0:2
m.formula <- as.formula(paste0(DV, " ~ 1 + logRT_precrit1 + (1|Subject) + (1|Item)"))
modelAndPlot("Intercept + Precritical RT",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = FALSE)
```

In a next step, Association is added as a further predictor to the model, again not leading to drastic improvements in fit:

```{r, results = FALSE, echo = FALSE, fig.width=9, fig.height=5}
m.formula <- as.formula(paste0(DV, " ~ 1 + logRT_precrit1 + Association + (1|Subject) + (1|Item)"))
modelAndPlot("Intercept + RT_precrit + Association",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = FALSE)
```

The next three plots add Coherence, Inference and InfCoPCA individually to the baseline model (Intercept + precritical RT), all three leading to improvements in model fit:

```{r, results = FALSE, echo = FALSE, fig.width=9, fig.height=5}
m.formula <- as.formula(paste0(DV, " ~ 1 +  logRT_precrit1 + Coherence + (1|Subject) + (1|Item)"))
modelAndPlot("Intercept + RT_precrit + Coherence",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = FALSE)
```

```{r, results = FALSE, echo = FALSE, fig.width=9, fig.height=5}
m.formula <- as.formula(paste0(DV, " ~ 1 +  logRT_precrit1 + Inference + (1|Subject) + (1|Item)"))
modelAndPlot("Intercept + RT_precrit + Inference",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = FALSE)
```

```{r, results = FALSE, echo = FALSE, fig.width=9, fig.height=5}
m.formula <- as.formula(paste0(DV, " ~ 1 +  logRT_precrit1 + InfCoPCA + (1|Subject) + (1|Item)"))
modelAndPlot("Intercept + RT_precrit + InfCoPCA",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = FALSE)
```

Next, Association + Coherence together are added to the model, first without an interaction term between them...

```{r, results = FALSE, echo = FALSE, fig.width=9, fig.height=5}
m.formula <- as.formula(paste0(DV, " ~ 1 + Association + Coherence + 
   logRT_precrit1 + (1 | Subject) + (1 | Item)"))
modelAndPlot("Full model (no interaction)",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = FALSE)
```

...and then with an interaction term between Association and Coherence, leading again to improved model fit (this is the full model):

```{r, results = FALSE, echo = FALSE, fig.width=9, fig.height=5}
m.formula <- as.formula(paste0(DV, " ~ 1 + Association + Coherence + 
   logRT_precrit1 +  Association*Coherence + (1|Subject) + (1|Item)"))
modelAndPlot("Full model (with interaction)",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = FALSE)
```

Next, we can also check if it helps to include Block, TrialNumber and/or Target noun length as predictors.
**However, as these factors were counterbalanced across lists / conditions, they are not hypothesized to lead to improvements in model fit.**

```{r results = FALSE, echo = FALSE}
# m.formula <- as.formula(paste0(DV, " ~ 1 + Association + Coherence + 
#    logRT_precrit1 + Block +
#    Association*Coherence + (1 | Subject) + (1 | Item)"))
# modelAndPlot("Full model with Block position as predictor",
#              m.formula,
#              df,
#              m.type,
#              regions,
#              DV,
#              y.unit,
#              y.range,
#              y.range.res,
#              coef.plot = FALSE)

m.formula <- as.formula(paste0(DV, " ~ 1 + Association + Coherence + 
   logRT_precrit1 + TrialNum +
   Association*Coherence + (1 | Subject) + (1 | Item)"))
modelAndPlot("Full model with Trial Number as predictor",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = FALSE)

# m.formula <- as.formula(logRT ~ Association + Coherence + logRT_precrit1 + Association*Coherence + TrialNum + Length + (1|Subject) + (1|Item))
# modelAndPlot("Full model with TrialNum and Target Length",
#              m.formula,
#              df,
#              m.type,
#              regions,
#              DV,
#              y.unit,
#              y.range,
#              y.range.res,
#              coef.plot = FALSE)
```


### B) Modeling subsets of the data

We now turn to assessing the impact of individual predictors and transformations thereof in subsets of the full data.
Each untransformed-logtransformed pair of predictors is analyzed in the subset of conditions where the pair shows maximal variation, while the other predictors remain constant.
For each pair, a minimal lmer model is constructed containing only the Intercept + the predictor, as well as random intercepts for Subject+Item. 

These data subsets were created:

- a subset containing conditions A & B is used to model the impact of the Production Norms
- a subset containing conditions C & D is used to model Association
- a subset containing conditions A & C is used to model the impact of Inference/Coherence/InfCoPCA (TODO: alternatively, use ABC data frame? => see last 3 plots)
- precritical RT is also tested on the A & C subset, mirroring the coherent vs. incoherent divide observed in the precritical region

The following plots show the model residuals with the untransformed variable on the left and the log-transformed variable on the right, respectively:

```{r, echo = FALSE, results = FALSE, warning = FALSE, error = FALSE, fig.width=7, fig.height=4}
# Create subset dfs:
df.AB <- filter(df, Cond %in% c("A", "B"))
df.AC <- filter(df, Cond %in% c("A", "C"))
df.CD <- filter(df, Cond %in% c("C", "D"))
df.ABC <- filter(df, Cond %in% c("A", "B", "C"))

compareLogToUntransformed(df.AB, "ProdNorms", "logProdNorms")
compareLogToUntransformed(df.AC, "RT_precrit1", "logRT_precrit1")
compareLogToUntransformed(df.CD, "Association", "logAssociation")

compareLogToUntransformed(df.AC, "Inference", "logInference")
compareLogToUntransformed(df.AC, "Coherence", "logCoherence")
compareLogToUntransformed(df.AC, "InfCoPCA", "logInfCoPCA")

#compareLogToUntransformed(df.ABC, "Inference", "logInference")
#compareLogToUntransformed(df.ABC, "Coherence", "logCoherence")
#compareLogToUntransformed(df.ABC, "InfCoPCA", "logInfCoPCA")
```

**Preliminary observations:**

- Log-transformation of the predictor variable does not lead to improved fit, except very slightly for precritical RT in regions 0-2 (as predicted). Therefore, the original (scaled) versions of the predictors will be kept, except for precritical RT.
- Subset conditions A + B:
  - The **production norms** can model the difference between A and B well except in the critical region
- Subset conditions A + C:
  - **Inference** can model the difference between A and C well except in the post-spillover region
  - **Coherence** can model the difference between A and C rather well in all regions
  - **InfCoPCA** can model the difference between A and C well in all regions
  - **Precritical region RT** alone cannot model the difference between A and C well (except in the precritical region itself)
- Subset conditions C + D:
  - **Association** can model the difference between C and D very well in the post-spillover region

**C) Compare the collinear predictors Inference, Coherence and InfCoPCA on the full set of conditions**

Use residual plots to compare the collinear predictors:

```{r, echo = FALSE, results = FALSE, fig.width=10, fig.height=4}
f1 <- as.formula(get(DV) ~ 1 + Inference + (1|Subject) + (1|Item))
df.m1 <- runModels(df, model.type="lmer", f1)
p1 <- plotSPR(df.m1, "Residuals", y.unit, y.range.res) + ggtitle("Residuals: Inference")
#ggsave("./plots/residuals_Inference.pdf", last_plot(), width=3.5, height=3)

f2 <- as.formula(get(DV) ~ 1 + Coherence + (1|Subject) + (1|Item))
df.m2 <- runModels(df, model.type="lmer", f2)
p2 <- plotSPR(df.m2, "Residuals", y.unit, y.range.res) + ggtitle("Residuals: Coherence")

f3 <- as.formula(get(DV) ~ 1 + logInfCoPCA + (1|Subject) + (1|Item))
df.m3 <- runModels(df, model.type="lmer", f3)
p3 <- plotSPR(df.m3, "Residuals", y.unit, y.range.res) + ggtitle("Residuals: InfCoPCA")

p <- grid.arrange(p1, p2, p3, nrow=1)
ggsave("./plots/residuals_Inf_Co_InfCoPCA.pdf", p, width=10, height=4)
```


```{r include = FALSE}
#AIC and BIC (f1 = Inference, f2 = Coherence, f3 = InfCoPCA):
#(Use only the critical and spillover region, exclude precritical.)

# AIC scores
getMeanAICBIC(df, f1, "AIC", regions=0:2)
getMeanAICBIC(df, f2, "AIC", regions=0:2)
getMeanAICBIC(df, f3, "AIC", regions=0:2)
# BIC scores
getMeanAICBIC(df, f1, "BIC", regions=0:2)
getMeanAICBIC(df, f2, "BIC", regions=0:2)
getMeanAICBIC(df, f3, "BIC", regions=0:2)

# **Result:** both mean AIC and mean BIC across regions is lowest for Coherence.
```

```{r include = FALSE}
#AIC and BIC for combined full models:
f1 <- as.formula(get(DV) ~ 1 + Association + Inference + Association*Inference + logRT_precrit1 + (1|Subject) + (1|Item))
f2 <- as.formula(get(DV) ~ 1 + Association + Coherence + Association*Coherence + logRT_precrit1 + (1|Subject) + (1|Item))
f3 <- as.formula(get(DV) ~ 1 + Association + InfCoPCA + Association*InfCoPCA + logRT_precrit1 + (1|Subject) + (1|Item))

# AIC scores
getMeanAICBIC(df, f1, "AIC", regions=0:2)
getMeanAICBIC(df, f2, "AIC", regions=0:2)
getMeanAICBIC(df, f3, "AIC", regions=0:2)
# BIC scores
getMeanAICBIC(df, f1, "BIC", regions=0:2)
getMeanAICBIC(df, f2, "BIC", regions=0:2)
getMeanAICBIC(df, f3, "BIC", regions=0:2)
```

## Neutralizing predictors

We start from the full chosen model: 

```{r, results = FALSE, echo = FALSE, fig.width=9, fig.height=5}
m.formula <- as.formula(paste0(DV, " ~ 1 + Association + Coherence + 
   logRT_precrit1 +  Association*Coherence + (1|Subject) + (1|Item)"))
modelAndPlot("Full model (with interaction)",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = FALSE)

# Get coefficient plot and effect size plot
m <- runModels(df, model.type="lmer", m.formula, regions=regions)
p.coef <- plotCoefs(m)
p.effect <- plotEffects(m)

p <- grid.arrange(p.coef, p.effect, nrow=1,
             top = textGrob("Full model: Coefficients + Effect size",
                            gp=gpar(fontsize=14)))

ggsave("./plots/coef_effectsize.pdf", plot = p,
       width = 10, height = 4, dpi = 300)
```

**Comparing by-subject linear model, lmer with / without random intercepts for items:**

```{r}
# Linear model by-subjects:
f1 <- as.formula(paste0(DV," ~ Association + Coherence + Association:Coherence + ", precrit1))
m1 <- runModels(df, "lm.by.subj", f1, regions)

# LMER without by-item random intercepts:
f2 <- as.formula(paste0(DV," ~ Association + Coherence + Association:Coherence + ", precrit1, " + (1|Subject)"))
m2 <- runModels(df, "lmer", f2, regions)

# LMER with by-item random intercepts:
f3 <- as.formula(paste0(DV," ~ Association + Coherence + Association:Coherence + ", precrit1, " + (1|Subject) + (1|Item)"))
m3<- runModels(df, "lmer", f3, regions)

#p1 <- plotSPR(m1, "Residuals", y.unit, c(-0.04, 0.04)) + ggtitle("By-subject linear model")
#p2 <- plotSPR(m2, "Residuals", y.unit, c(-0.04, 0.04)) + ggtitle("LMER: no by-item intercept")
#p3 <- plotSPR(m3, "Residuals", y.unit, c(-0.04, 0.04)) + ggtitle("LMER: by-item intercept")
p1 <- plotSPR(m1, "Residuals", y.unit, y.range.res) + ggtitle("By-subject linear model")
p2 <- plotSPR(m2, "Residuals", y.unit, y.range.res) + ggtitle("LMER: no by-item intercept")
p3 <- plotSPR(m3, "Residuals", y.unit, y.range.res) + ggtitle("LMER: by-item intercept")
p <- grid.arrange(p1, p2, p3, nrow=1,
                  top = textGrob(paste0("Residuals: linear/lmer model with/without by-item random intercepts"),
                                 gp=gpar(fontsize=14, fontface=2)))
ggsave(paste0("./plots/residual_comparison_item_random_intercepts.pdf"), p, 
       width = 10, height = 5, dpi = 300)
```

### Setting predictors to zero

**Currently the following plots are still not the lmer models!**

**Model formula for the full model:**

*logRT ~ Association + Coherence + Association:Coherence + Precritical_RT*

```{r, include = FALSE, fig.height = 7, fig.width = 14}
m.formula <- as.formula(paste0(
  DV,
  " ~ Association + Coherence + Association:Coherence + ",
  precrit1
))

m <- runModels(df,
               "lm.by.subj",
               m.formula,
               regions)

# Set predictors to zero: start with all zero except the intercept,
# then add in more predictors

# Plot only the intercept
m$InterceptOnly <- m$`Coef_(Intercept)`
plotSPR(m, "InterceptOnly", y.unit, y.range)

# Add association only
m$InterceptAssoc <- m$`Coef_(Intercept)` + m$Association*m$Coef_Association 
plotSPR(m, "InterceptAssoc", y.unit, y.range)

# Add coherence only
m$InterceptCoherence <- m$`Coef_(Intercept)` + m$Coherence*m$Coef_Coherence
plotSPR(m, "InterceptCoherence",  y.unit, y.range)

# Add precritical only 
m$InterceptPrecrit1 <- m$`Coef_(Intercept)` + m$logRT_precrit1*m$Coef_logRT_precrit1
plotSPR(m, "InterceptPrecrit1",  y.unit, y.range)
```

```{r, fig.height = 8, fig.width = 10}
### Generate plots 

# Add everything except association 
m$InterceptCoherencePrecrit1Interaction <- m$`Coef_(Intercept)` + m$Association*0 + m$Coherence*m$Coef_Coherence + m$logRT_precrit1*m$Coef_logRT_precrit1 + m$Association*m$Coherence*m$`Coef_Association:Coherence`
p1 <- plotSPR(m, "InterceptCoherencePrecrit1Interaction",
        y.unit, y.range) + ggtitle("Association removed")

# Add everything except coherence 
m$InterceptAssocPrecrit1Interaction <- m$`Coef_(Intercept)` + m$Association*m$Coef_Association + m$Coherence*0 + m$logRT_precrit1*m$Coef_logRT_precrit1 + m$Association*m$Coherence*m$`Coef_Association:Coherence`
p2 <- plotSPR(m, "InterceptAssocPrecrit1Interaction",
        y.unit, y.range) + ggtitle("Coherence removed")

# Add everything except precritical 1 
m$InterceptAssocCoherenceInteraction <- m$`Coef_(Intercept)` + m$Association*m$Coef_Association + m$Coherence*m$Coef_Coherence + m$logRT_precrit1*0 + m$Association*m$Coherence*m$`Coef_Association:Coherence`
p3 <- plotSPR(m, "InterceptAssocCoherenceInteraction", 
        y.unit, y.range)  + ggtitle("Precritical RT removed")

# Add everything except the interaction term 
m$InterceptAssocCoherencePrecrit1 <- m$`Coef_(Intercept)` + m$Association*m$Coef_Association + m$Coherence*m$Coef_Coherence + m$logRT_precrit1*m$Coef_logRT_precrit1 + m$Association*m$Coherence*0
p4 <- plotSPR(m, "InterceptAssocCoherencePrecrit1",
        y.unit, y.range)  + ggtitle("Interaction term removed")

# Sanity check: full model 
m$Full <- m$`Coef_(Intercept)` + m$Association*m$Coef_Association  + m$Coherence*m$Coef_Coherence + m$logRT_precrit1*m$Coef_logRT_precrit1 +  m$Association*m$Coherence*m$`Coef_Association:Coherence`
p5 <- plotSPR(m, "Full", y.unit, y.range) + ggtitle("FULL MODEL")

# Observed data 
p.observed <- plotSPR(m, DV,  y.unit, y.range) + ggtitle("OBSERVED RTS")

grid.arrange(p1, p2, p3, 
             p4, p5, p.observed,
             nrow=2,
             top = textGrob("Setting individual predictors to zero", gp=gpar(fontsize=14, fontface=2)))
```


```{r echo = FALSE}
knitr::knit_exit()
```

