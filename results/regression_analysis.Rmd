---
title: "SPR results: regression-based analysis of the reading times"
author: "Miriam Schulz"
date: "10/11/2022"
output:
  html_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      collapse = TRUE,
                      fig.height = 6, fig.width = 9)
```


# About

This script contains a regression-based analysis of the inference SPR data based on the rERP framework (Smith & Kutas 2015a, 2015b) as adapted and generalized to reading times by Brouwer, Delogu & Crocker (2021) and Aurnhammer et al. (2021).


# Read data and preprocess

In this section:

- read in the data
- remove outliers based on 
    - SPR (word-by-word) RTs
    - context sentence RTs
- remove incorrectly answered trials (plausibility ratings)
- compute interaction terms
- add log-transformed RTs
- standardize predictors
- inspect and visualize the SPR and context sentence RTs

```{r include = FALSE}
# Workspace and libraries
rm(list = ls())
library(tidyverse)
library(lme4)              # for lmer()
library(lmerTest)          # for summary(lmer.model)
library(broom)             # for tidy(modeloutput)
library(gridExtra)         # for grid.arrange()
library(grid)              # for textGrob()
library(RColorBrewer)      # for color palettes
library(pander)            # for pander(df)
source("functions.R")      # custom preprocessing and helper functions
source("functions_rRT.R")  # rRT functions
options(dplyr.summarise.inform = FALSE)  # remove message in output
```

Read in data and remove fillers, practice and non-critical regions, extract sentence-final word:

```{r message=FALSE}
# Read data
df <- read.csv("results_reads.csv", header = TRUE)

# Exclude filler and practice trials
df <- removeFillersPractice(df)

# Extract sentence-final word
dfinal <- data.frame(aggregate(WordNum ~ Item,
          data = df, 
          FUN = max))
colnames(dfinal) <- c("Item", "Final")
df <- merge(df, dfinal)
df.final <- filter(df, WordNum == Final)
final <- aggregMeans(df.final, as.formula(RT ~ Cond))
df$Final <- NULL

# Exclude non-critical regions
df <- removeRegions(df, keep.regions=-1:2)
```

Next, the raw data are visualized with histograms and QQ plots before removing outliers.

Histograms and residual plots:

```{r echo=FALSE, fig.width=10, fig.height=10}
par(mfrow=c(2,2))
# Histograms

# Histogram of RTs:
hist(df$RT,
     breaks = 50,
     col='steelblue',
     xlab = "RTs (ms)",
     main='Untransformed RTs')

# Histogram of log RTs :
hist(log(df$RT),
     breaks = 50,
     col='steelblue',
     xlab = "logRTs (ms)",
     main='Log-transformed RTs')

# QQ plots

# Untransformed RTs
qqnorm(df$RT,
       col='steelblue',
       pch=16,
       main='QQ plot of untransformed RTs')

# Log-transformed RTs
qqnorm(log(df$RT),
       col='steelblue',
       pch=16,
       main='QQ plot of log-transformed RTs')
# Save plot
ggsave("./plots/RT_hist_QQ_preDataExclusion.pdf", plot = last_plot(),
       width = 10, height = 10, dpi = 300)
par(mfrow=c(1,1))  # reset
```

## Remove SPR outliers

```{r message=FALSE}
# Optional: remove items with low accuracy in B
#df <- filter(df, ! Item %in% c(5,13,25))

# Remove outliers: SPR (word RTs) - SD
df <- removeOutliersSPR(df,
                        method = "sd",
                        sd.value = 4,
                        entire.trial = TRUE,
                        regions = -1:2)

# Check the remaining range of data points remaining from each subject:
range(xtabs(~ Subject, data=df))
# This shows that between 0 and 16 datapoints were removed per subject. 

# Uncomment to test the impact of excluding the fastest + slowest subject
#df <- filter(df, !Subject %in% c(6, 20))
```

Histograms and residual plots *after* removing SPR outliers:

```{r echo=FALSE, fig.width=10, fig.height=10}
par(mfrow=c(2,2))
# Histograms

# Histogram of RTs:
hist(df$RT,
     breaks = 50,
     col='steelblue',
     xlab = "RTs (ms)",
    main='Untransformed RTs')

# Histogram of log RTs :
hist(log(df$RT),
     breaks = 50,
     col='steelblue',
     xlab = "logRTs (ms)",
     main='Log-transformed RTs')

# QQ plots

# Untransformed RTs
qqnorm(df$RT,
       col='steelblue',
       pch=16,
       main='QQ plot of untransformed RTs')

# Log-transformed RTs
qqnorm(log(df$RT),
       col='steelblue',
       pch=16,
       main='QQ plot of log-transformed RTs')
# Save plot
ggsave("./plots/RT_hist_QQ_postDataExclusion.pdf", plot = last_plot(),
       width = 10, height = 10, dpi = 300)
par(mfrow=c(1,1))  # reset
```

Normality test (Shapiro-Wilk):

```{r}
# Untransformed RTs:
shapiro.test(filter(df, Region >= 0)$RT)

# Log-transformed RTs:
shapiro.test(log(filter(df, Region >= 0)$RT))
```

## Remove context sentence RT outliers and incorrectly answered trials

```{r fig.width=10, fig.height=5}
# Remove outliers: context sentence reading times
df <- removeOutliersContext(df, min.rt=500, max.rt=30000)

# Remove trials with incorrect plausibility ratings
df <- removeIncorrect(df)

# Visualize context sentence reading times with a histogram:
par(mfrow=(c(1,2)))
hist(
    filter(df, Region == 0)$ContextRT,
    breaks = 50,
    main = "Untransformed Context Sentence RTs",
    xlab = "Context RTs (ms)",
    col = "steelblue"
    # main = NULL
)
hist(
    log(filter(df, Region == 0)$ContextRT),
    breaks = 50,
    main = "Log-transformed Context Sentence RTs",
    xlab = "Context RTs (log-ms)",
    col = "steelblue"
)
par(mfrow=(c(1,1))) # reset
```

```{r echo=FALSE, fig.width=4, fig.height=3.5}
# Visualize average context reading times by condition:
cond.means.contextRTs <- aggregMeans(df,
                                     as.formula(ContextRT ~ Cond))
round(cond.means.contextRTs$Mean)
round(cond.means.contextRTs$SE)

p <-
  cond.means.contextRTs %>% 
  ggplot(aes(x = Cond, y = Mean, fill=Cond)) + 
  geom_bar(stat='identity', width=0.8) + 
  geom_errorbar(
    aes(ymin = Mean - SE,
        ymax = Mean + SE),
    width = 0.15,
    linewidth = 0.3
  ) +
  #ylim(0, 4000) +
  xlab("Condition") +
  ylab("RT (ms)") +
  scale_fill_manual(values = c("cornflowerblue", "chartreuse3",
                               "tomato2", "darkgoldenrod1")) +
  theme_minimal() +
  theme(legend.position="None",
        text = element_text(size = 13))
filename <- paste0("./plots/", "context_RT.pdf")
ggsave(filename, plot = p, width = 3, height = 3)
p + ggtitle("Context sentence RT by condition")
```

## Further preprocessing: transformations etc.

In this subsection: 

- Rename variables
- Transform to factor when necessary
- Add precritical reading times as predictors 
- Add log-transformed versions of the reading times (the SPR RT (DV) as well as predictors (IVs))
- Computer interaction terms
- Standardize the predictors

```{r message=FALSE}
# Rename variables 
rating_colnames <- str_remove(names(df), "Rating")
names(df) <- rating_colnames
df <- rename(df, ProdNorms = ProductionPercentage)

# Transform variables to factors
df$Cond <- as.factor(df$Cond)
df$Item <- as.factor(as.numeric(as.character(df$Item)))
df$Subject <- as.factor(as.numeric(as.character(df$Subject)))

# Create binary predictors for coherence and association
df$BinaryCoherence <- as.factor(ifelse(df$Plausible == "Yes", "coherent", "incoherent"))
df$BinaryAssociation <- as.factor(ifelse(df$Cond %in% c("A", "C"), "associated", "unassociated"))

# Extract precritical RT -1 and add to df:
df.precrit <- filter(df, Region == -1) %>% 
  dplyr::select(Item, Cond, Subject, RT) %>% 
  rename(RT_precrit = RT)
df <- merge(df, df.precrit, by = c("Item", "Cond", "Subject"))

# Add target length 
df <- df %>% mutate(Length = nchar(Target))

# Add log-transformed variables (DV + IVs)
df$logRT <- log(df$RT)
df$logRT_precrit <- log(df$RT_precrit)
df$logProdNorms <- ifelse(df$ProdNorms > 0,
                          log(df$ProdNorms), 0.01)  # smoothe for 0
df$logAssociation <- log(df$Association)
df$logInference <- log(df$Inference)
df$logCoherence <- log(df$Coherence)

# Add interaction variables
df$AssocInference <- df$Association*df$Inference
df$AssocCoherence <- df$Association*df$Coherence

# Order df by Region
df <- arrange(df, Region)

# Scale predictors (careful: needs to be performed AFTER removing data)
df <- scalePredictors(
  df,
  c(
    "ProdNorms",
    "Association",
    "Inference",
    "Coherence",
    "AssocInference",
    "AssocCoherence",
    "RT_precrit",
    "logRT_precrit",
    "Length",
    "logProdNorms",
    "logAssociation",
    "logInference",
    "logCoherence"
  )
)

#str(df)
```


# Observed RTs

## Mean RTs by condition for each region of interest

```{r echo = FALSE}
# Summary:
# Untransformed RTs
pander::pander(df %>%
                 aggregMeans(as.formula(RT ~ Region + Cond)) %>% 
                 select(-SD, -SE) %>% 
                 spread(Region, Mean) %>%
                 mutate_if(is.numeric, round) %>% 
                 rename("precritical" = "-1",
                        "critical" = "0", 
                        "spillover1" = "1", 
                        "spillover2" = "2"),
               caption = "Untransformed RTs")
# Log-transformed RTs
pander::pander(df %>%
                 aggregMeans(as.formula(logRT ~ Region + Cond)) %>% 
                 select(-SD, -SE) %>% 
                 spread(Region, Mean) %>% 
                 rename("precritical" = "-1",
                        "critical" = "0", 
                        "spillover1" = "1", 
                        "spillover2" = "2"),
               caption = "Log-transformed RTs")
```

## Plots

### RTs by region and condition 

```{r echo = FALSE, fig.width=6.5, fig.height=4.5}
#df <- filter(df, !Item %in% c(5, 13, 25))  # optional: remove items
p1 <- plotSPR(df, DV="RT", y.unit="RT", y.range=c(260, 325))
p2 <- plotSPR(df, DV="logRT", y.unit="logRT", y.range=c(5.52,5.73))
ggsave("./plots/observed_RTs.pdf", p2,
       width = 3.2, height = 3.5, dpi=300)
grid.arrange(p1+ggtitle("Untransformed RTs"),
             p2+ggtitle("Log-transformed RTs"),
             ncol=2,
             top = textGrob("Observed reading times",
                            gp=gpar(fontsize=14, fontface=2)))
```


### Density plot: spread in each region

Check the *spread* of the variation of the reading times in the critical regions.

Larger ranges implies that the residuals will be larger due to the increased amount of variation in that region.

```{r echo = FALSE, warning = FALSE, fig.width=6, fig.height=4}
spread.color.palette <- c("-1" = "rosybrown1",
                          "0" = "black",
                          "1" = "deeppink2", 
                          "2" = "purple3")
# Density plot
p.spread <- df %>%
  filter(Region %in% -1:2) %>%
  ggplot(aes(
    x = logRT,
    group = Region,
    linetype = factor(Region),
    color = factor(Region)
  )) +
  geom_density(adjust = 0.8,
               linewidth = 1,
               key_glyph = draw_key_smooth) +
  #xlim(c(0, 1000)) +
  ylab("Density") +
  scale_color_manual("Region",  # Legend title
                     values = spread.color.palette,
                     labels = c("Pre-critical", "Critical",
                                "Spillover", "Post-spillover")) +
  scale_linetype_manual(
    "Region",  # Legend title
    values = c("dotdash", "solid", "dashed", "dotted"),
    labels = c("Pre-critical", "Critical",
               "Spillover", "Post-spillover")
  ) +
  theme_minimal()

ggsave("./plots/spread_RT.pdf", p.spread,
       width = 5, height = 3.5, dpi=300)

p.spread + ggtitle("Spread of RTs by Region")
```

### Task effect: decreasing RTs over time

Check the presence of a task effect, i.e., decreasing RTs over the course of the experiment.

**Only critical region (target noun) RTs are plotted.**

*(Note that across items, reversed list orders counterbalanced for trial position effects!)*

**Method 1: RT ~ Block**

```{r echo = FALSE, fig.width=3, fig.height=2.5}
mean.RT.block <- aggregMeans(filter(df, Region==0),
                             as.formula(RT ~ Block))

p.taskeffect <- mean.RT.block %>%
  ggplot(aes(x = Block, y = Mean, color = Block)) +
  geom_point(size = 5,
             shape = "cross") +
  geom_errorbar(aes(ymin = Mean - SE,
                    ymax = Mean + SE),
                width = 0.2,
                linewidth = 0.3) +
  xlab("Time (Block)") +
  ylab("RT (ms)") +
  scale_colour_brewer(palette="Dark2")+
  theme_minimal() + 
  theme(legend.position="None")

ggsave("./plots/taskeffect.pdf", p.taskeffect,
       width = 3, height = 2.5, dpi=300)

p.taskeffect + ggtitle("Mean RT by block position")
```


**Method 2: RT ~ TrialNumber**

```{r echo = FALSE, message = FALSE, fig.width=7, fig.height=4}
mean.RT.trialnum <- df %>% 
  filter(Region == 0) %>% 
  group_by(TrialNum) %>% 
  summarize(MeanRT = mean(RT)) %>% 
  data.frame()

mean.RT.trialnum %>% 
  ggplot(aes(x=TrialNum, y=MeanRT)) + 
  geom_smooth(method = 'lm', se=TRUE, alpha = .2, color='steelblue') + 
  geom_line(color='steelblue') + 
  #ylim(c(0,350)) + 
  xlab("Time (trial position)") + 
  ylab("RT (ms)") +
  theme_minimal() +
  ggtitle("Mean RT by trial position")
```

```{r include = FALSE, fig.width=6, fig.height=4}
df %>%
  filter(Region == 0) %>% 
  ggplot(aes(x = TrialNum, y = RT, color=Subject, group=Subject)) + 
  #geom_point(size=1.5, shape="cross", alpha = 0.5) + 
  geom_smooth(method = 'lm', size = 0.5, se=TRUE, alpha = .2, aes(fill=Subject)) +
  #geom_smooth(method = 'lm', se=TRUE, alpha = .2, aes(x = TrialNum, y = RT)) +
  ggtitle("RT by Subject + trial number") +
  theme_minimal() +
  theme(legend.position = "None")
```


# Traditional analysis

Run a separate lmer model for each region with contrast-coded categorical predictor values and observe p-values.

(The precritical region is not modeled, since precritical RT is used as a predictor.)

```{r}
# Set contrasts
#contrasts(df$Cond) <- car::contr.Sum(levels(df$Cond))
#contrasts(df$Cond)
contrasts(df$BinaryCoherence) <- contr.sum(2)
contrasts(df$BinaryAssociation) <- contr.sum(2)

contrasts(df$BinaryCoherence)
contrasts(df$BinaryAssociation)

# Split data into different data frames for each region 
crit <- filter(df, Region==0)
spill1 <- filter(df, Region==1)
spill2 <- filter(df, Region==2)

#f <- as.formula(logRT ~ Cond + logRT_precrit + (1|Subject) + (1|Item))
f <- as.formula(logRT ~ BinaryCoherence*BinaryAssociation + logRT_precrit + (1|Subject) + (1|Item))

#######################
### CRITICAL REGION ###
#######################

summary(lmer(f, data=crit))

##############################
### FIRST SPILLOVER REGION ### 
##############################

summary(lmer(f, data=spill1))

###############################
### SECOND SPILLOVER REGION ### 
###############################

summary(lmer(f, data=spill2))
```


```{r include = FALSE}
# Compare this model to a model without the interaction term:
f1 <- as.formula(logRT ~ BinaryAssociation*BinaryCoherence + logRT_precrit + (1|Subject) + (1|Item))
f2 <- as.formula(logRT ~ BinaryAssociation+BinaryCoherence + logRT_precrit + (1|Subject) + (1|Item))
# AIC
getMeanAICBIC(df, f1, "AIC", regions=0:2)
getMeanAICBIC(df, f2, "AIC", regions=0:2)
# BIC
getMeanAICBIC(df, f1, "BIC", regions=0:2)
getMeanAICBIC(df, f2, "BIC", regions=0:2)
```

# rRT analysis

In the following analyses, each observed RT is replaced with an estimated RT obtained from a linear mixed effects model.
Beyond analyzing the estimated RTs, this approach allows to examine how the estimates change dynamically as individual predictors are either a) incrementally added to the model (forward model construction), or b) controlled for by setting them to their grand mean value across conditions, which lets us inspect the contribution of individual predictors to goodness of fit on a by-condition and by-region basis.

In the first part of the analysis, forward model construction is used to investigate the impact of individual predictors and perform model selection.
This approach allows to contrast untransformed with log-transformed versions of the same predictor, and to compare the collinear predictors Inference, Coherence and their interactions.

In the second part of the analysis, a technique based on neutralizing predictors in the full model is then used to gain insight into how the individual predictors -- corresponding to specific properties of the stimulus presented at trial $i$ -- combine at a latent level in each trial to form the resulting RT.

Concretely, the signal for a specific stimulus at each trial $i$ is broken down into a weighted sum of the individual predictors, plus the noise term $\epsilon_i$:

$$
\begin{align}
y_i &=  \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} +  ... + \epsilon_i \\
& = \beta_0 + \sum_{j = 1}^N \beta_j x_{ji} + \epsilon_i
\end{align}
$$

where 

- $\beta_0$ is the intercept;
- $\beta_j$ stands for the coefficient estimate for an individual predictor $j$;
- $x_{ji}$ stands for the stimulus property for predictor $j$ at trial $i$;
- $\epsilon_i$ stands for the noise in trial $i$

The goal of the estimation process is to find the set of most optimal $\hat{\beta}_j$'s that minimizes $\epsilon_i$ across trials, given the stimulus properties $x_{ji}$.

The impact of each predictor in each region of interest can then be analyzed by inspecting the change in the residual error ensuing when the predictor is controlled for (i.e., when its coefficient is set to its mean value -- which in the present analysis corresponds to setting it to zero, given that predictors were standardized to have a mean of $0$ and a standard deviation of $1$).
More precisely, an increase in residual error when controlling for a specific predictor in a model (and correspondingly a larger deviation of the estimated RTs from the observed RTs) indicates that this predictor is critical for reducing the error in that region.
This provides a quantitative measure of goodness of model fit that allows to compare different models in which the impact of the individual predictors can be isolated (where closer approximation of $\epsilon$ to $0$ means better fit).

Finally, the **effect structure** of the *observed RTs* and that of the *estimated rRTs* can be analyzed and compared using statistical analysis:

- First, lmer models can be run on the estimated rRTs in each region, providing estimates for the coefficients and effect size just like analysis of the observed rRTs.
- Second, an anova can be run to compare two versions of the final model, one using the observed RTs and the other using the estimated rRTs, with type as a between-subjects factor, where no significant effect of type is expected if the rRTs adequately capture the variance in the observed data.

General points:

- In the following, the different regions are analyzed as *independent models* following the hypothesis that the predictors will have different effects in different regions. 
To control for baseline offset, the precritical region is always added as a predictor in the models for the different regions (except in the baseline intercept model). 
- Note that inclusion of precritical region RT as a predictor in the model for the precritical region itself leads to perfect estimates with a singular fit warning and no remaining error, as is to be expected.
The precritical region is therefore not modelled and plotted in models that include precritical RT as a predictor.


## Define parameters

Define general parameters:

- regions of interest to model
- untransformed or log-transformed RTs
- based on RT unit (ms or log ms): ranges for the plot y-axis
- which kind of models to run:
  - "lm": simple LM
  - "lm.by.subj": by-subject LMs
  - "lmer": LMER

```{r}
regions <- -1:2
log.RTs <- TRUE
m.type = "lmer"

if (log.RTs == TRUE) {
  DV <- "logRT"
  precrit <- "logRT_precrit"
  y.unit <- "Log RT"
  y.range <- c(5.52, 5.72)
  y.range.res <- c(-0.07, 0.07)
} else {
  DV <- "RT"
  precrit <- "RT_precrit"
  y.unit <- "RT (ms)"
  y.range <- c(260, 320)
  y.range.res <- c(-20, 20)
}

if (m.type == "lmer") {
  rand.ef <- " + (1|Subject) + (1|Item)"
} else {
  rand.ef = ""
}
```

**For the final analyses, lmer models with log-transformed RTs as the DV were used.**


## Forward model construction

This section aims to assess the fit of individual predictors in isolation, starting from a simple baseline model containing only the intercept.

**Aims:**

A) provide preliminary insights into the variance in the signal, and the impact of each predictor in isolation; then gradually build models by adding predictors.
B) determine the impact of log-transformations of the predictors for the following predictors: Production Norms, Association, Inference, Coherence (and precriticalRT?)

**Method:**

A) begin with a baseline (intercept only) model, then add the individual predictors of interest one by one
B) compare residual plots for different versions of the *same* predictor (e.g. untransformed Association vs. log-transformed Association), using a subset of the experimental conditions for which there is variation in this predictor while variation is minimized among the other predictors

### A) From baseline to full model

Following the logic of Brouwer et al. (2020), the analysis starts with a baseline model of the variance in the signal. 
The baseline model assumes that no factors were manipulated and that all trials belong to the same condition. 
Therefore, the resulting estimate for each trial in a given region will simply be the average over all trials in that region, with slight offsets due to the random effects. 
Building on this baseline model, predictors are then added in isolation until we arrive at the full model including an interaction term.

**Note: In this section, I used lmer models with random intercepts for Subject and Item, but no random slopes, as the latter consistently led to convergence problems.**

```{r, results = FALSE, echo = FALSE, fig.width=9, fig.height=5}
# Run model(s) and generate plots
m.formula <- makeFormula(DV,
                         IVs = "1",
                         rand.ef = rand.ef)
modelAndPlot("Intercept",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = FALSE)
```

Next, the logRTs on the precritical region are included as a predictor in the baseline model to account for the baseline offset observed in the precritical region.

The pre-critical region will therefore no longer be modeled and plotted in the remainder of this section, since its RTs are used as a predictor, always leading to perfect fit in the precritical region.

For the other regions, the residual plot shows that including precritical RT alone as a predictor does not notably improve model fit:

```{r, results = FALSE, echo = FALSE, fig.width=9, fig.height=5}
# change the default regions: remove precritical from models 
regions <- 0:2
m.formula <- makeFormula(DV,
                         IVs = "1 + logRT_precrit",
                         rand.ef = rand.ef)
modelAndPlot("Intercept + Precritical RT",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = FALSE)
```

In a next step, Association is added as a further predictor to the model, again not leading to drastic improvements in fit:

```{r, results = FALSE, echo = FALSE, fig.width=9, fig.height=5}
m.formula <- makeFormula(DV,
                         IVs = "1 + logRT_precrit + Association",
                         rand.ef = rand.ef)
modelAndPlot("Intercept + RT_precrit + Association",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = TRUE)
```

The next two plots add Coherence and Inference individually to the baseline model (Intercept + precritical RT), both leading to similar improvements in model fit:

```{r, results = FALSE, echo = FALSE, fig.width=9, fig.height=5}
m.formula <- makeFormula(DV,
                         IVs = "1 + logRT_precrit + Coherence",
                         rand.ef = rand.ef)
modelAndPlot("Intercept + RT_precrit + Coherence",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = TRUE)
```

```{r, results = FALSE, echo = FALSE, fig.width=9, fig.height=5}
m.formula <- makeFormula(DV,
                         IVs = "1 + logRT_precrit + Inference",
                         rand.ef = rand.ef)
modelAndPlot("Intercept + RT_precrit + Inference",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = TRUE)
```

We can also add both Inference and Coherence to the baseline model (on the assumption that collinearity does not pose a problem to linear regression per se, without performing inferential statistics):

```{r, results = FALSE, echo = FALSE, fig.width=9, fig.height=5}
m.formula <- makeFormula(DV,
                         IVs = "1 + logRT_precrit + Inference + Coherence",
                         rand.ef = rand.ef)
modelAndPlot("Intercept + RT_precrit + Inference + Coherence",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = TRUE)

# VIF scores per region:
for (r in regions) {
  m <- lmer(m.formula, data=filter(df, Region == r))
  print(car::vif(m))
}
```

So far, we have focused on assessing the influence of individual predictors in isolation (as well as the correlated Inference+Coherence). 

In the next step, we can add Association to the previously constructed model (still following Smith & Kutas 2015 in keeping collinear predictors despite the potential for variance inflation):

```{r, results = FALSE, echo = FALSE, fig.width=9, fig.height=5}
m.formula <- makeFormula(DV,
                         IVs = "1 + logRT_precrit + Inference + Coherence + Association",
                         rand.ef = rand.ef)
modelAndPlot("Intercept + RT_precrit + Inference + Coherence + Association",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = TRUE)
# VIF scores per region:
for (r in regions) {
  m <- lmer(m.formula, data=filter(df, Region == r))
  print(car::vif(m))
}
```

Finally, we can also add the hypothesized interaction term for Association:Inference to the model (note that the interaction term is computed from the scaled predictors, instead of being computed individually from the raw predictors and then scaled as a product, since the latter approach leads to huge inflation of variance in the model coefficients):

```{r, results = FALSE, echo = FALSE, fig.width=9, fig.height=5}
m.formula <- makeFormula(DV,
                         IVs = "1 + logRT_precrit + Coherence +
                         Association*Inference",
                         rand.ef = rand.ef)
modelAndPlot("Intercept + RT_precrit + Inf + Co + Assoc + Assoc:Inf",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = TRUE)

for (r in regions) {
  m <- lmer(m.formula, data=filter(df, Region == r))
  print(car::vif(m))
}
```

```{r, include = FALSE, echo = FALSE, fig.width=9, fig.height=5}
# In addiion, we can also add ContextRT, DecisionRT and TrialNumber to test
# if this has an impact on the model predictions & effect sizes:
m.formula <- makeFormula(DV,
                         IVs = "1 + logRT_precrit + Coherence +
                         Association*Inference  + scale(ContextRT) +
                         scale(DecisionRT) + scale(TrialNum)",
                         rand.ef = rand.ef)
modelAndPlot("Intercept + RT_precrit + Inf + Co + Assoc + Assoc:Inf",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = TRUE)
```

This shows an interesting pattern in the coefficient and effect size plots: while Inference is determined by the model as having a positive-going and significant effect in the first spillover region and Coherence is not, this is reversed in the post-spillover region where Coherence is predicted to have a positive, significant effect while the effect of Inference even becomes numerically negative from the model's point of view.

This then demonstrates how, when given collinear predictors, the model can exploit which predictor best maximizes fit in each region, and place positive or negative weights accordingly, irrespective of the correlation between the predictors.

This phenomenon however also makes the models more prone to overfit to noise in the data.

As a sanity check, we can also compute a model in which we remove the collinear predictor Coherence, to check if the effect of Inference then reverses in the post-spillover region:

```{r, results = FALSE, echo = FALSE, fig.width=9, fig.height=5}
m.formula <- makeFormula(DV,
                         IVs = "1 + logRT_precrit +
                         Association*Inference",
                         rand.ef = rand.ef)
modelAndPlot("Intercept + RT_precrit + Inf + Assoc + Assoc:Inf",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = TRUE)

for (r in regions) {
  m <- lmer(m.formula, data=filter(df, Region == r))
  print(car::vif(m))
}
```

As speculated, the reversal of the sign for Inference is exactly what is observed in the post-critical region, with a significant, positive effect for this predictor. 

As another sanity check, we can also check if it helps to include  TrialNumber as a predictor.
However, as trial order of items and conditions was counterbalanced across lists, this should not lead to improvements in model fit, even if it will be a hightly significant predictor.

```{r, results = FALSE, echo = FALSE, fig.width=9, fig.height=5}
df$TrialNum <- scale(df$TrialNum)
m.formula <- makeFormula(DV,
                         IVs = "1 + logRT_precrit + Coherence +
                         Association*Inference + TrialNum",
                         rand.ef = rand.ef)
modelAndPlot("Intercept + RT_precrit + Inf + Co + Assoc + Assoc:Inf + TrialNum",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = TRUE)
```


### B) Modeling subsets of the data

We now turn to assessing the impact of individual predictors and transformations thereof in subsets of the full data.
Each untransformed-logtransformed pair of predictors is analyzed in the subset of conditions where the pair shows maximal variation, while the other predictors remain constant.
For each pair, a minimal lmer model is constructed containing only the Intercept + the predictor, as well as random intercepts for Subject+Item. 

These data subsets were created:

- a subset containing conditions A & B is used to model the impact of the Production Norms
- a subset containing conditions C & D is used to model Association
- a subset containing conditions A & C is used to model the impact of Inference/Coherence (TODO: alternatively, use ABC data frame? => see last 3 plots)
- precritical RT is also tested on the A & C subset, mirroring the coherent vs. incoherent divide observed in the precritical region

The following plots show the model residuals with the untransformed variable on the left and the log-transformed variable on the right, respectively:

```{r echo=FALSE, error=FALSE, fig.height=4, fig.width=7, warning=FALSE, results=FALSE}
# Create subset dfs:
df.AB <- filter(df, Cond %in% c("A", "B"))
df.AC <- filter(df, Cond %in% c("A", "C"))
df.CD <- filter(df, Cond %in% c("C", "D"))

compareLogToUntransformed(df.AB, "ProdNorms", "logProdNorms")
compareLogToUntransformed(df.CD, "Association", "logAssociation")
compareLogToUntransformed(df.AC, "Inference", "logInference")
compareLogToUntransformed(df.AC, "Coherence", "logCoherence")
compareLogToUntransformed(df.AC, "RT_precrit", "logRT_precrit")

# Plot an intercept model for the subset dataframes
f <- makeFormula(DV,
                 IVs = "1",
                 rand.ef = rand.ef)
modelAndPlot("Intercept model on data subset: Conditions A+B",
             f,
             df.AB,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = FALSE)
```

**Observations:**

- Log-transformation of the predictor variable does not lead to improved fit, except very slightly for precritical RT in regions 0-2 (as predicted). Therefore, the original (scaled) versions of the predictors will be kept, except for precritical RT.
- Subset conditions A + B:
  - The **production norms** can model the difference between A and B well except in the critical region
- Subset conditions A + C:
  - **Inference** can model the difference between A and C well except in the post-spillover region
  - **Coherence** can model the difference between A and C rather well in all regions
  - **Precritical region RT** alone cannot model the difference between A and C well (except in the precritical region itself)
- Subset conditions C + D:
  - **Association** can model the difference between C and D very well in the post-spillover region


## Neutralizing predictors

### Full model + coefficients and effect size

We start from the full chosen model: 

```{r, results = FALSE, echo = FALSE, fig.width=9, fig.height=5}
m.formula <- makeFormula(DV,
                         IVs = "1 + logRT_precrit + Coherence + Association +
                         Inference + Association:Inference",
                         rand.ef = rand.ef)
modelAndPlot("Full model (with interaction)",
             m.formula,
             df,
             m.type,
             regions,
             DV,
             y.unit,
             y.range,
             y.range.res,
             coef.plot = TRUE)
```

**Comparing by-subject linear model, lmer with / without random intercepts for items:**

```{r}
# Linear model by-subjects:
f1 <- makeFormula(DV, IVs = "1 + logRT_precrit + Coherence + Association*Inference",
                         rand.ef = "")
m1 <- runModels(df, "lm.by.subj", f1, regions)

# LMER without by-item random intercepts:
f2 <- makeFormula(DV, IVs = "1 + logRT_precrit + Coherence + Association*Inference",
                         rand.ef = " + (1|Subject)")
m2 <- runModels(df, "lmer", f2, regions)

# LMER with by-item random intercepts:
f3 <- makeFormula(DV, IVs = "1 + logRT_precrit + Coherence + Association*Inference",
                         rand.ef = " + (1|Subject) + (1|Item)")
m3 <- runModels(df, "lmer", f3, regions)

#p1 <- plotSPR(m1, "Residuals", y.unit, c(-0.04, 0.04)) + ggtitle("By-subject linear model")
#p2 <- plotSPR(m2, "Residuals", y.unit, c(-0.04, 0.04)) + ggtitle("LMER: no by-item intercept")
#p3 <- plotSPR(m3, "Residuals", y.unit, c(-0.04, 0.04)) + ggtitle("LMER: by-item intercept")
p1 <- plotSPR(m1, "Residuals", y.unit, y.range.res) + ggtitle("By-subject linear model")
p2 <- plotSPR(m2, "Residuals", y.unit, y.range.res) + ggtitle("LMER: subject intercept")
p3 <- plotSPR(m3, "Residuals", y.unit, y.range.res) + ggtitle("LMER: subject + item intercepts")
p <- grid.arrange(p1, p2, p3, nrow=1,
                  top = textGrob(paste0("Residuals: linear/lmer model with/without by-item random intercepts"),
                                 gp=gpar(fontsize=14, fontface=2)))
ggsave(paste0("./plots/residual_comparison_item_random_intercepts.pdf"), p, 
       width = 10, height = 5, dpi = 300)
```


### Controlling individual predictors

**Model formula for the full model:**

*logRT ~ logRT_precrit + Association + Coherence + Inference + Association:Inference*

```{r, fig.height = 3, fig.width = 6}
m.formula <- makeFormula(DV,
                         IVs = "1 + logRT_precrit + Coherence + Association +
                         Inference + Association:Inference",
                         rand.ef = rand.ef)

# Specify which random effects to include when generating model predictions
rand.ef.cols <- c("RanEfSubject", "RanEfItem")

m <- runModels(df,
               m.type,
               m.formula,
               regions)

# Specify full model IVs:
model.ivs <- c(precrit, "Association", "Inference", "Coherence",
               "Association:Inference")

# Readjust the residuals y-axis (make larger, since larger residuals arise)
y.range.res <- c(-0.11, 0.11)

# Sanity check 1: remove ALL predictors (yields intercept model)
neutralizePredictors(m,
                     neutralize=model.ivs,
                     model.ivs,
                     rand.ef.cols = rand.ef.cols)

# Sanity check 2: remove all predictors except precritical RT
neutralizePredictors(m,
                     neutralize=model.ivs[! model.ivs == precrit],
                     model.ivs,
                     rand.ef.cols = rand.ef.cols)


### Neutralizing individual predictors
neutralizePredictors(m, neutralize=c("Association"), model.ivs,
                     rand.ef.cols = rand.ef.cols)
neutralizePredictors(m, neutralize=c("Coherence"), model.ivs,
                     rand.ef.cols = rand.ef.cols)
neutralizePredictors(m, neutralize=c("Inference"), model.ivs,
                     rand.ef.cols = rand.ef.cols)
neutralizePredictors(m, neutralize=c("Association:Inference"), model.ivs,
                     rand.ef.cols = rand.ef.cols)
neutralizePredictors(m, neutralize=precrit, model.ivs,
                     rand.ef.cols = rand.ef.cols)

### Isolating predictors (ie. neutralizing everything else except intercept)

# Keep only Association
neutralizePredictors(m,
                     neutralize=model.ivs[! model.ivs == "Association"],
                     model.ivs,
                     rand.ef.cols = rand.ef.cols)

# Keep only Inference
neutralizePredictors(m,
                     neutralize=model.ivs[! model.ivs == "Inference"],
                     model.ivs,
                     rand.ef.cols = rand.ef.cols)

# Keep only Coherence
neutralizePredictors(m,
                     neutralize=model.ivs[! model.ivs == "Coherence"],
                     model.ivs,
                     rand.ef.cols = rand.ef.cols)

# Keep only Interaction
neutralizePredictors(m,
                     neutralize=model.ivs[! model.ivs ==
                                            "Association:Inference"],
                     model.ivs,
                     rand.ef.cols = rand.ef.cols)


### Isolating pairs of variables 
# Keep only Inference + Coherence
neutralizePredictors(m,
                     neutralize=model.ivs[! model.ivs %in%
                                           c("Inference", "Coherence")],
                     model.ivs,
                     rand.ef.cols = rand.ef.cols)
```


